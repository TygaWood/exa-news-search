[
  {
    "score": 0.18087561428546906,
    "title": "NovelAI on X: \"[NovelAI V4]\nA little teaser for all those who love cats! Stay on the look out for more teasers of NovelAI's next image generation model. https://t.co/P03mD6vrO9\" / X",
    "id": "https://x.com/novelaiofficial/status/1867692454440579386",
    "url": "https://x.com/novelaiofficial/status/1867692454440579386",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "",
    "text": "None\n              Conversation                                      10:05 PM · Dec 13, 2024                                                         By signing up, you agree to the   and  , including",
    "summary": "NovelAI shared a teaser image of their upcoming V4 image generation model on X (formerly Twitter).  The teaser features cats.  No further details on release date or specific features were provided.\n"
  },
  {
    "score": 0.17164717614650726,
    "title": "NovelAI on X: \"[Image Generation]\n\"Holiday Spirit\" Toggle has returned!\nInfuse your AI Image generations with instant holiday cheer! \nActivate the \"Add some holiday spirit\" toggle to generate holiday-cheer-infused images!\n\nThe NovelAI Team https://t.co/wPbavKsvP7\" / X",
    "id": "https://x.com/novelaiofficial/status/1867714177835667533",
    "url": "https://x.com/novelaiofficial/status/1867714177835667533",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "",
    "text": "Activate the \"Add some holiday spirit\" toggle to generate holiday-cheer-infused images!\nThe NovelAI Team https://t.co/wPbavKsvP7\" / X\n2024-12-13\nNone\n                                                         Conversation                                      11:32 PM · Dec 13, 2024                                                   By signing up, you agree to the   and  , including",
    "summary": "NovelAI has re-enabled its \"Add some holiday spirit\" toggle for image generation.  This allows users to generate images with a holiday theme.\n"
  },
  {
    "score": 0.16788804531097412,
    "title": "",
    "id": "https://twitter.com/OpenAI/status/1867628181974290752",
    "url": "https://twitter.com/OpenAI/status/1867628181974290752",
    "publishedDate": "2024-12-13T17:50:32.000Z",
    "author": "OpenAI",
    "text": "Day 7: Projects in ChatGPT—a new way to organize and customize your chats.\nhttps://t.co/Dt7wzatS6l| created_at: Fri Dec 13 17:50:32 +0000 2024 | favorite_count: 1631 | quote_count: 103 | reply_count: 251 | retweet_count: 169 | is_quote_status: False | retweeted: False | lang: en",
    "summary": "OpenAI announced \"Projects in ChatGPT,\" a new feature for organizing and customizing chats.  This was tweeted on December 13, 2024.\n"
  },
  {
    "score": 0.16715846955776215,
    "title": "Arm on X: \".@AIatMeta's Llama 3.3 70B model is changing the game for GenAI text generation.\n\nIn our recent benchmarking on Arm Neoverse-powered Google Axion processors, we saw how impactful this new model is in bringing GenAI text generation capabilities to everyone: https://t.co/LBIUwBHAcd https://t.co/gdFjsqPxRb\" / X",
    "id": "https://x.com/Arm/status/1867587301049508198",
    "url": "https://x.com/Arm/status/1867587301049508198",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "",
    "text": "2024-12-13\nNone\n             Conversation                                      okt.to/tHRqXA           3:08 PM · Dec 13, 2024",
    "summary": "Arm announced today that Meta's Llama 3.3 70B model is significantly improving generative AI text capabilities, particularly on Arm Neoverse-powered Google Axion processors.  This makes advanced GenAI text generation more accessible.\n"
  },
  {
    "score": 0.16491107642650604,
    "title": "ChatGPT Projects are fancy folders for your AI chats",
    "id": "https://www.theverge.com/2024/12/13/24320800/openai-chatgpt-projects-folders-ai-chats",
    "url": "https://www.theverge.com/2024/12/13/24320800/openai-chatgpt-projects-folders-ai-chats",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Jay Peters",
    "text": "OpenAI is rolling out a feature called “Projects” to ChatGPT. It’s basically a folder system that makes it easier to organize things you’re working on while using the AI chatbot.  As shown in a demo video, your list of Projects will show up in the sidebar. If you make a new project, you can do things like edit the title, set a color for the project’s icon, and add files as well as instructions to tailor how ChatGPT responds to things in that individual project. You can also add previous chats to your project to keep track of them. The new feature seems like a pretty useful way to keep track of, for lack of a better word, your projects. During the demo video, an OpenAI employee showed examples of how they use Projects to plan for a Secret Santa gift exchange and for home maintenance. Depending on your needs, it could be a better way to work on a project than my usual method, which is dumping everything I can think of into an Apple Note. Projects is rolling out today to ChatGPT Plus, Pro, and Teams users. It will come to free users “as soon as possible” and to Enterprise and Edu users “early in the new year,” according to OpenAI CPO Kevin Weil.   Projects was announced as Day 7 of OpenAI’s 12 days of “ship-mas.” Previous announcements included the release of the Sora video generator, ChatGPT’s Canvas view, and the $200-per-month ChatGPT Pro subscription.",
    "summary": "OpenAI released ChatGPT Projects, a new organizational feature for ChatGPT Plus, Pro, and Teams users.  This allows users to create projects with custom titles, colors, and associated files and instructions, helping manage ongoing AI chats.  The feature will roll out to free users soon and to Enterprise/Edu users early next year.  This is part of OpenAI's \"12 days of ship-mas,\" which also included the Sora video generator and updates to ChatGPT's Canvas view.\n"
  },
  {
    "score": 0.1644728183746338,
    "title": "",
    "id": "https://twitter.com/novelaiofficial/status/1867714177835667533",
    "url": "https://twitter.com/novelaiofficial/status/1867714177835667533",
    "publishedDate": "2024-12-13T23:32:15.000Z",
    "author": "novelaiofficial",
    "text": "[Image Generation]\n\"Holiday Spirit\" Toggle has returned!\nInfuse your AI Image generations with instant holiday cheer!\nActivate the \"Add some holiday spirit\" toggle to generate holiday-cheer-infused images!\nThe NovelAI Team https://t.co/wPbavKsvP7| created_at: Fri Dec 13 23:32:15 +0000 2024 | favorite_count: 20 | quote_count: 0 | reply_count: 1 | retweet_count: 3 | is_quote_status: False | retweeted: False | lang: en",
    "summary": "NovelAI has re-enabled its \"Holiday Spirit\" toggle for AI image generation, allowing users to create holiday-themed images.\n"
  },
  {
    "score": 0.16418881714344025,
    "title": "",
    "id": "https://twitter.com/novelaiofficial/status/1867692454440579386",
    "url": "https://twitter.com/novelaiofficial/status/1867692454440579386",
    "publishedDate": "2024-12-13T22:05:55.000Z",
    "author": "novelaiofficial",
    "text": "[NovelAI V4]\nA little teaser for all those who love cats! Stay on the look out for more teasers of NovelAI's next image generation model. https://t.co/P03mD6vrO9| created_at: Fri Dec 13 22:05:55 +0000 2024 | favorite_count: 136 | quote_count: 8 | reply_count: 5 | retweet_count: 34 | is_quote_status: False | retweeted: False | lang: en",
    "summary": "NovelAI has released a teaser for its next image generation model, V4, featuring cats.  No further details on release date or features were provided.\n"
  },
  {
    "score": 0.16316717863082886,
    "title": "OpenAI launches Projects, a ChatGPT feature to organize files and chats into folders, now available to Plus, Pro, and Team users and “soon” to free users",
    "id": "https://www.techmeme.com/241213/p29",
    "url": "https://www.techmeme.com/241213/p29",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "",
    "text": "",
    "summary": "OpenAI has released \"Projects,\" a new ChatGPT feature allowing Plus, Pro, and Team users to organize their chats and files into folders.  This feature will soon be available to free users.\n"
  },
  {
    "score": 0.161588653922081,
    "title": "NotebookLM gets a new look, audio interactivity and a premium version",
    "id": "https://blog.google/technology/google-labs/notebooklm-new-features-december-2024/",
    "url": "https://blog.google/technology/google-labs/notebooklm-new-features-december-2024/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Steven Johnson",
    "text": "Dec 13, 2024\n[[read-time]] min read\nWe’re rolling out a new product interface, letting you “join” Audio Overviews to speak to the hosts and introducing NotebookLM Plus, our subscription product.\n NotebookLM is the ultimate tool for understanding the information that matters most to you. Globally, millions of people and tens of thousands of organizations are using our AI-powered research assistant. They’re studying smarter, making informed decisions faster and listening on the go with Audio Overviews. Today, we’re starting to roll out the experimental version of Gemini 2.0 Flash in NotebookLM and more updates:  A new interface, optimized for managing and generating new content based on your sources The ability to engage directly with the AI hosts during an Audio Overview NotebookLM Plus, our premium version for power users, teams and enterprises, with new features and higher usage limits  \n Manage and generate content in a new, intuitive way From the start, we wanted NotebookLM to be a tool that would let you move effortlessly from asking questions to reading your sources to capturing your own ideas. Today, we’re rolling out a new design that makes it easier than ever to switch between those different activities in a single, unified interface. The redesign organizes NotebookLM into three areas. The \"Sources\" panel manages all the information that’s central to your project. The \"Chat\" panel lets you discuss your sources through a conversational AI interface with citations. The \"Studio\" panel lets you create new things from your sources with a single click, like Study Guides, Briefing Docs and Audio Overviews. This design also fluidly adapts to your needs. For example, you can expand the source viewer and notes editor side by side if you want to take notes on an important document, or you can ask questions in the chat while listening to an Audio Overview. \n NotebookLM's new interface simplifies switching between managing, reading and generating content from your sources, all in one place \n Join and interact with the AI hosts in Audio Overviews Over the last three months, people have generated more than 350 years worth of Audio Overviews. Today, we’re starting to roll out the ability to “join” the conversation. Using your voice, you can ask the hosts for more details or to explain a concept differently. It’s like having a personal tutor or guide who listens attentively, and then responds directly, drawing from the knowledge in your sources. Here’s how you can use it:  Create a new Audio Overview. Tap the new Interactive mode (BETA) button and hit play. While listening, tap \"Join.\" A host will call on you. Ask your question!  \n \n\"Join\" your Audio Overviews to speak with the hosts, ask questions and get custom responses based on your sources.\nThis is an experimental feature and only works on new Audio Overviews. Hosts may also pause awkwardly before responding and may occasionally introduce inaccuracies. We're gradually rolling this out so you can try it early and give us feedback on Discord. Try it out and let us know what you think.\n Try NotebookLM Plus for your organization We’re launching a subscription plan for NotebookLM, and it’s called NotebookLM Plus. Subscribers will receive a number of benefits, including more than five times more Audio Overviews, notebooks and sources per notebook; the ability to customize the style and length of your notebook responses; shared team notebooks with usage analytics; and additional privacy and security. Starting today, NotebookLM Plus will be available with enterprise-grade protection for businesses, schools and universities, organizations and enterprise customers via Google Workspace or can be purchased separately via Google Cloud. It will also be included in Google One AI Premium starting in early 2025. Explore all of the benefits and pricing on our website. \n NotebookLM Plus comes with a new set of features and higher usage limits, including five times more Audio Overviews, notebooks, and sources per notebook \nWe hope you enjoy these updates and follow us on X for the latest product updates and behind-the-scenes stories from the team.",
    "summary": "Google announced several updates to NotebookLM, its AI-powered research assistant.  These include a redesigned interface for easier content management and generation, interactive Audio Overviews allowing users to engage with AI hosts, and the launch of a premium subscription, NotebookLM Plus, offering enhanced features and higher usage limits.  The interactive Audio Overviews are currently in beta.\n"
  },
  {
    "score": 0.16123034060001373,
    "title": "OpenAI on X: \"Introducing Projects—an easy way to organize chats that share topics or context in 4o.\n\nNow available for ChatGPT Plus, Pro, and Team users globally.\n\nWe’ll bring it to Enterprise and Edu users in January, and to Free users soon. https://t.co/Bmv7kB0GKY\" / X",
    "id": "https://x.com/OpenAI/status/1867675796950987146",
    "url": "https://x.com/OpenAI/status/1867675796950987146",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "",
    "text": "We’ll bring it to Enterprise and Edu users in January, and to Free users soon. https://t.co/Bmv7kB0GKY\" / X\n2024-12-13\nNone\n             Conversation                                            8:59 PM · Dec 13, 2024",
    "summary": "OpenAI announced \"Projects,\" a new feature for ChatGPT Plus, Pro, and Team users to organize chats by topic or context.  It will roll out to Enterprise and Education users in January 2025, and free users at a later date.\n"
  },
  {
    "score": 0.1601327806711197,
    "title": "Apple Intelligence in iOS 18.2 & Apple Vision Pro is the gadget of the year on the AppleInsider Podcast",
    "id": "https://appleinsider.com/articles/24/12/13/apple-intelligence-in-ios-182-apple-vision-pro-is-the-gadget-of-the-year-on-the-appleinsider-podcast?utm_medium=rss",
    "url": "https://appleinsider.com/articles/24/12/13/apple-intelligence-in-ios-182-apple-vision-pro-is-the-gadget-of-the-year-on-the-appleinsider-podcast?utm_medium=rss",
    "publishedDate": "2024-12-13T13:37:09.000Z",
    "author": "William Gallagher",
    "text": "With this week's OS updates, Apple Intelligence is truly here, with more to come. Plus Apple is again or still in trouble for what's it done and what it hasn't, all on the AppleInsider Podcast.\n Of course we've still more Apple Intelligence to come, and in particular the much-awaited series of improvements to Siri. But now that iOS 18.2 and macOS Sequoia 15.2, not to mention iPadOS 18.2, it feels as if Apple Intelligence has truly arrived. And the first thing it did was bring down OpenAI's ChatGPT servers because of the demand.\n So anecdotally, it looks as if a lot of iPhone, Mac and iPad users are at least trying out Apple Intelligence. The question now will be whether they find it useful, or whether it only has that brief new toy kind of appeal.\n But at least you can listen to three new Apple Music radio stations while you try to get Image Playground to make an accurate, okay reasonable, okay recognizable avatar of yourself. And while you read about the new lawsuits that are trying to get Apple to bring back its abandoned CSAM plans, or to do something more about deepfakes.\n  BONUS: Subscribe via Patreon or Apple Podcasts to hear AppleInsider+, the extended edition. This time, William wants to confess about how after 10 hours working at a Mac, sometimes he just relaxes by doing something else on that same screen. It's about how Apple pervades — and maybe in a good way — takes over our hobbies as well as our work.\n   @WGallagher on Twitter \n William's 58keys on YouTube \n William Gallagher on email \n @hillithreads on Threads \n Wes on Bluesky \n Wes Hilliard on email    MasterClass: take advantage of the new special discount offers on annual memberships at MasterClass.com   Links from the Show   Wedding banned - Apple Watches not welcome at fashionista's ceremony \n Apple Music Radio taps big talent for three new stations \n Apple's iOS 18.2 release brings Image Playground to the iPhone \n New Genmoji ad showcases creations that definitely were not made with Apple Intelligence \n Adobe Lightroom, Balatro+ honored in Apple's App Store Awards \n Apple's iPhone 17 Slim is a wrongheaded approach that ignores what people really want \n New iPhone 17 rumors point to a radically different rear camera layout \n Congress asking Apple and other big tech what they're doing about deepfakes \n Apple's ongoing modem push rumored to result in cellular MacBooks \n Apple plans three-year rollout of its self-made iPhone modem, starting with iPhone SE \n Apple has reportedly approached Sony to partner on Apple Vision Pro controllers \n Analysis: Apple Vision Pro sells well, but needs more content faster \n Half of Apple developers have doubts about making Apple Vision Pro apps \n Apple Vision Pro named innovation of the year, beating transparent TVs and AI cheese \n Apple sued over 2022 dropping of CSAM detection features \n Congress asking Apple and other big tech what they're doing about deepfakes \n The death of a robot designed for autistic children proves Apple's on-device AI is the right path   More AppleInsider podcasts Tune in to our HomeKit Insider podcast covering the latest news, products, apps and everything HomeKit related. Subscribe in Apple Podcasts, Overcast, or just search for HomeKit Insider wherever you get your podcasts.\n Podcast artwork from Basic Apple Guy. Download the free wallpaper pack here.\n Those interested in sponsoring the show can reach out to us at: advertising@appleinsider.com.\n Subscribe to AppleInsider on:   Apple Podcasts \n Overcast \n Pocket Casts \n Castro \n Stitcher \n Google Podcasts    Keep up with everything Apple in the weekly AppleInsider Podcast. Just say, \"Hey, Siri,\" to your HomePod mini and ask for these podcasts, and our latest HomeKit Insider episode too. If you want an ad-free main AppleInsider Podcast experience, you can support the AppleInsider podcast by subscribing for $5 per month through Apple's Podcasts app, or via Patreon if you prefer any other podcast player.",
    "summary": "Apple's iOS 18.2, iPadOS 18.2, and macOS Sequoia 15.2 updates include the release of Apple Intelligence,  a new AI feature.  Early anecdotal evidence suggests high user interest, though its long-term usefulness remains to be seen.  The update also includes three new Apple Music radio stations and the Image Playground app.\n",
    "image": "https://photos5.appleinsider.com/gallery/62040-128485-000-lede-Apple-Intelligence-on-iPhone-xl.jpg",
    "favicon": "https://photos5.appleinsider.com/v10/images/favicon-ai-32.png"
  },
  {
    "score": 0.15867234766483307,
    "title": "Steven Johnson on X: \"Major update rolling out today at NotebookLM: a completely new adaptive design; interact with the Audio Overview hosts; and NotebookLM Plus--our premium version with expanded features.\n\nFull features below 👇\n\nNew flexible \"3-panel\" interface lets you easily switch between asking https://t.co/VRBLgDPcAZ\" / X",
    "id": "https://x.com/stevenbjohnson/status/1867593824832503883",
    "url": "https://x.com/stevenbjohnson/status/1867593824832503883",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "",
    "text": "New flexible \"3-panel\" interface lets you easily switch between asking https://t.co/VRBLgDPcAZ\" / X\n2024-12-13\nNone",
    "summary": "NotebookLM, an AI tool, has received a major update.  Key improvements include a new three-panel interface for easier interaction, the ability to interact with Audio Overview hosts, and the launch of a premium version, NotebookLM Plus, with expanded features.\n"
  },
  {
    "score": 0.15856385231018066,
    "title": "Google’s NotebookLM AI podcast hosts can now talk to you, too",
    "id": "https://www.theverge.com/2024/12/13/24318099/google-notebooklm-audio-overviews-talk-plus",
    "url": "https://www.theverge.com/2024/12/13/24318099/google-notebooklm-audio-overviews-talk-plus",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "",
    "text": "Google’s NotebookLM and its podcast-like Audio Overviews have been a surprise hit this year, and today Google company is starting to roll out a big new feature: the ability to actually talk with the AI “hosts” of the overviews. When the feature is available to you, you can try it out with new Audio Overviews. (It won’t work with old ones.) Here’s how, according to a blog post:   Create a new Audio Overview. Tap the new Interactive mode (BETA) button. While listening, tap “Join.” A host will call on you. Ask your question. The hosts will respond with a personalized answer based on your sources. After answering, they’ll resume the original Audio Overview.   The ability to actually talk with NotebookLM seems like a potentially useful way to learn more about what you’ve collected in the app. But Google cautions that it’s an “experimental feature” and that “hosts may also pause awkwardly before responding or occasionally introduce inaccuracies,” so it may not be a totally polished experience to start.  In addition to the interactive Audio Overviews, Google is introducing a new interface for NotebookLM that organizes things into three areas: a “sources” panel for your information, a “chat” panel to talk with an AI chatbot about the sources, and a “studio” panel that lets you make things like Audio Overviews and Study Guides. I think it looks nice.             GIF: Google   Google is announcing a NotebookLM subscription, too: NotebookLM Plus. The subscription will give you “five times more Audio Overviews, notebooks, and sources per notebook,” let you “customize the style and tone of your notebook responses,” let you make shared team notebooks, and will offer “additional privacy and security,” Google says. The subscription is available today for businesses, schools and universities, and organizations and enterprise customers. It will be added to Google One AI Premium in “early 2025.” Google is also launching “Agentspace,” a platform for custom AI agents for enterprises. “Agentspace can provide conversational assistance, answer complex questions, make proactive suggestions and take actions based on your company’s unique information,” Google says. It also has connectors for apps like Microsoft SharePoint, Jira, and ServiceNow.",
    "summary": "Google announced several AI updates today, including interactive Audio Overviews for NotebookLM.  Users can now converse with the AI host to ask questions about their collected information, although Google notes this is experimental and may have inaccuracies.  Additionally, a NotebookLM Plus subscription offers increased capacity and customization options, available now for organizations and coming to Google One AI Premium in early 2025.  Finally, Google launched Agentspace, a platform for creating custom AI agents for enterprises, integrating with various apps like Microsoft SharePoint and Jira.\n",
    "image": "https://cdn.vox-cdn.com/thumbor/EaScurH_lVme4X1artA78Qh3lpo=/0x0:2040x1360/1200x628/filters:focal(1020x680:1021x681)/cdn.vox-cdn.com/uploads/chorus_asset/file/24016888/STK093_Google_01.jpg",
    "favicon": "https://www.theverge.com/icons/favicon_32x32.png"
  },
  {
    "score": 0.15838150680065155,
    "title": "In a blog post, OpenAI says “You can't sue your way to AGI”, and publishes emails from Elon Musk showing he wanted to own OpenAI and run it as a for-profit",
    "id": "https://www.techmeme.com/241213/p23",
    "url": "https://www.techmeme.com/241213/p23",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "",
    "text": "",
    "summary": "The provided text is empty; therefore, no summary about the latest AI updates can be generated.  The title mentions a blog post from OpenAI discussing lawsuits and Elon Musk's past attempts to acquire and commercialize OpenAI, but this is unrelated to new AI releases.\n"
  },
  {
    "score": 0.15760141611099243,
    "title": "ChatGPT now can see what you see and explain it in Santa's voice",
    "id": "https://www.phonearena.com/news/chatgpt-can-see-what-you-see-santa-voice_id165817",
    "url": "https://www.phonearena.com/news/chatgpt-can-see-what-you-see-santa-voice_id165817",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Tsveta Ermenkova",
    "text": "OpenAI, the creator of ChatGPT, has been on a roll lately (or should I say, for the past few years?). Just recently, the company launched its 12 days of 12 livestreams event, revealing a new feature or product every day. And now, it has dropped another one. OpenAI has introduced two new features for ChatGPT. First up, the company has finally rolled out the real-time video capabilities it teased almost seven months ago. During one of its livestreams, OpenAI revealed that Advanced Voice Mode – ChatGPT’s conversational feature that mimics human-like responses – is now equipped with vision.  If you're using the ChatGPT app and subscribed to Plus, Team, or Pro, you can simply point your phone at an object, and ChatGPT will give you an almost instant response. No need to upload photos or type out detailed descriptions anymore.  What’s more, Advanced Voice Mode with vision can also interpret what’s on your device’s screen through screen sharing. Whether it’s explaining your settings menu or walking you through a tricky math problem, it’s got you covered. Once it rolls out, if you want to try it out, follow these steps:   Tap the voice icon next to the chat bar in the app. Hit the video icon at the bottom left to start using video.   Meanwhile, for screen sharing, just tap the three-dot menu and select “Share Screen.” OpenAI says the rollout for Advanced Voice Mode with vision kicks off Thursday and should be completed within a week. However, not everyone will get to try it right away. ChatGPT Enterprise and Edu subscribers will have to wait until January, and there’s no set timeline yet for users in the EU, Switzerland, Iceland, Norway, or Liechtenstein. \n \n   And as you can see from the video, OpenAI is getting into the holiday spirit. There is one more feature now: Santa mode. It’s a festive preset voice for ChatGPT that users can access by tapping or clicking the snowflake icon next to the app prompt bar.      Say ho ho ho to Santa in Voice Mode  Santa is rolling out today to everyone across all ChatGPT platforms and is available until the end of the month…then he will retire back to the North Pole. pic.twitter.com/NVS9bRok4r  — OpenAI (@OpenAI) December 12, 2024   I think that by rolling out features like video and screen sharing, OpenAI is pulling ahead of its main rival, Google, which just unveiled its latest AI model, Gemini 2.0. The battle between these tech giants is indeed pushing AI innovation at lightning speed. \n \n \nTsveta, a passionate technology enthusiast and accomplished playwright, combines her love for mobile technologies and writing to explore and reveal the transformative power of tech. From being an early follower of PhoneArena to relying exclusively on her smartphone for photography, she embraces the immense capabilities of compact devices in our daily lives. With a Journalism degree and an explorative spirit, Tsveta not only provides expert insights into the world of gadgets and smartphones but also shares a unique perspective shaped by her diverse interests in travel, culture, and visual storytelling.",
    "summary": "OpenAI released two new ChatGPT features: real-time video capabilities (using phone camera or screen share) for Plus, Team, and Pro subscribers (Enterprise and Edu users get it in January, EU users later), and a \"Santa\" voice mode available until the end of December.  The video feature allows ChatGPT to instantly analyze what's shown to it.\n"
  },
  {
    "score": 0.15674670040607452,
    "title": "ChatGPT can now see through your phone's camera and screen — one of its most impressive features yet",
    "id": "https://www.businessinsider.com/openai-chatgpt-can-now-see-through-phone-camera-and-screen-2024-12",
    "url": "https://www.businessinsider.com/openai-chatgpt-can-now-see-through-phone-camera-and-screen-2024-12",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Ana Altchek",
    "text": "OpenAI launched its widely anticipated video feature for ChatGPT's Advanced Voice Mode. It allows users to incorporate live video and screen sharing into conversations with ChatGPT. ChatGPT can interpret emotions, assist with homework, and provide real-time visual context.   ChatGPT's Advanced Voice Mode can now help provide real-time design tips for your home, assistance with math homework, or instant replies to your texts from the Messages app. After teasing the public with a glimpse of the chatbot's ability to \"reason across\" vision along with text and audio during OpenAI's Spring Update in May, the company finally launched the feature on Thursday as part of day six of OpenAI's \"Shipmas.\" \"We are so excited to start the rollout of video and screen share in Advanced Voice today,\" the company said in the livestream on Thursday. \"We know this is a long time coming.\" OpenAI initially said the voice and video features would be rolling out in the weeks after its Spring Update. However, Advanced Voice Mode didn't end up launching to users until September, and the video mode didn't come out until this week. The new capabilities help provide more depth in conversations with ChatGPT by adding \"realtime visual context\" with live video and screen sharing. Users can access the live video by selecting the Advanced Voice Mode icon in the ChatGPT app and then choosing the video button on the bottom far left.\n \nIn the livestream demonstration on Thursday, ChatGPT helped an OpenAI employee make pour-over coffee. The chatbot noticed details like what the employee was wearing and then walked him through the steps of making he drink, elaborating on certain parts of the process when asked. The chatbot also gave him feedback on his technique. To share your screen with ChatGPT, hit the drop-down menu and select \"Share Screen.\" In the \"Shipmas\" demo, ChatGPT could identify that the user was in the Messages app, understand the message sent, and then help formulate a response after the user asked. During the company's Spring Update, OpenAI showed off some other uses of the video mode. The chatbot was able to interpret emotions based on facial expressions and also demonstrated its ability to act as a tutor. OpenAI Research Lead Barret Zoph walked through an equation on a whiteboard (3x+1=4) and ChatGPT provided him with hints to find the value of x. The feature had a couple of stumbles during the Spring Update demonstration, like referring to one of the employees as a \"wooden surface\" or trying to solve a math problem before it was shown.\n \nNow that it's out, we decided to give the feature a whirl — and so far, it seems pretty impressive. We showed the chatbot an office plant and asked it to tell us about it, give context on whether it's healthy, and explain what the watering schedule should look like. The chatbot accurately described browning and drying on the leaf tips and identified it as an Aloe Vera plant, which seems to fit the right description. The new video feature will be rolling out this week in the latest version of the ChatGPT mobile app to Team and most Plus and Pro users. The feature isn't available in the EU, Switzerland, Iceland, Norway, and Liechtenstein yet, but OpenAI said it will be as soon as possible.",
    "summary": "OpenAI released a new video and screen-sharing feature for ChatGPT's Advanced Voice Mode.  This allows ChatGPT to interpret visual context in real-time, assisting with tasks like providing design tips, helping with homework, and even interpreting emotions from facial expressions.  The feature is rolling out this week to most Plus and Pro users (excluding the EU, Switzerland, Iceland, Norway, and Liechtenstein).\n"
  },
  {
    "score": 0.15497130155563354,
    "title": "Bringing AI Agents to Enterprises with Google Agentspace",
    "id": "https://cloud.google.com/blog/products/ai-machine-learning/bringing-ai-agents-to-enterprises-with-google-agentspace",
    "url": "https://cloud.google.com/blog/products/ai-machine-learning/bringing-ai-agents-to-enterprises-with-google-agentspace",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Saurabh Tiwary",
    "text": "AI &amp; Machine Learning",
    "summary": "The provided text is only HTML markup and contains no actual text content about AI updates.  Therefore, I cannot provide a summary of latest AI updates based on this input.  To answer your search query, please provide text content from the article.\n"
  },
  {
    "score": 0.15496714413166046,
    "title": "OpenAI blames its massive ChatGPT outage on a ‘new telemetry service’",
    "id": "https://techcrunch.com/2024/12/13/openai-blames-its-massive-chatgpt-outage-on-a-new-telemetry-service/",
    "url": "https://techcrunch.com/2024/12/13/openai-blames-its-massive-chatgpt-outage-on-a-new-telemetry-service/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Kyle Wiggers",
    "text": "OpenAI is blaming one of the longest outages in its history on a “new telemetry service” gone awry. \nOn Wednesday, OpenAI’s AI-powered chatbot platform, ChatGPT; its video generator, Sora; and its developer-facing API experienced major disruptions starting at around 3 p.m. Pacific. OpenAI acknowledged the problem soon after — and began working on a fix. But it’d take the company roughly three hours to restore all services.\nIn a postmortem published late Thursday, OpenAI wrote that the outage wasn’t caused by a security incident or recent product launch, but by a telemetry service it deployed Wednesday to collect Kubernetes metrics. Kubernetes is an open source program that helps manage containers, or packages of apps and related files that are used to run software in isolated environments.\n“Telemetry services have a very wide footprint, so this new service’s configuration unintentionally caused … resource-intensive Kubernetes API operations,” OpenAI wrote in the postmortem. “[Our] Kubernetes API servers became overwhelmed, taking down the Kubernetes control plane in most of our large [Kubernetes] clusters.”\nThat’s a lot of jargon, but basically, the new telemetry service affected OpenAI’s Kubernetes operations, including a resource that many of the company’s services rely on for DNS resolution. DNS resolution converts IP addresses to domain names; it’s the reason you’re able to type “Google.com” instead of “142.250.191.78.”\nOpenAI’s use of DNS caching, which holds info about previously-looked-up domain names (like website addresses) and their corresponding IP addresses, complicated matters by “delay[ing] visibility,” OpenAI wrote, and “allowing the rollout [of the telemetry service] to continue before the full scope of the problem was understood.”\nOpenAI says that it was able to detect the issue “a few minutes” before customers ultimately started seeing an impact, but that it wasn’t able to quickly implement a fix because it had to work around the overwhelmed Kubernetes servers. \n“This was a confluence of multiple systems and processes failing simultaneously and interacting in unexpected ways,” the company wrote. “Our tests didn’t catch the impact the change was having on the Kubernetes control plane [and] remediation was very slow because of the locked-out effect.”\nOpenAI says that it’ll adopt several measures to prevent similar incidents from occurring in the future, including improvements to phased rollouts with better monitoring for infrastructure changes and new mechanisms to ensure OpenAI engineers can access the company’s Kubernetes API servers in any circumstances.\n“We apologize for the impact that this incident caused to all of our customers – from ChatGPT users to developers to businesses who rely on OpenAI products,” OpenAI wrote. “We’ve fallen short of our own expectations.”\nMost Popular\n \nKyle Wiggers is a senior reporter at TechCrunch with a special interest in artificial intelligence. His writing has appeared in VentureBeat and Digital Trends, as well as a range of gadget blogs including Android Police, Android Authority, Droid-Life, and XDA-Developers. He lives in Brooklyn with his partner, a piano educator, and dabbles in piano himself. occasionally — if mostly unsuccessfully.\t\nView Bio \nNewsletters\nSubscribe for the industry’s biggest tech news\nRelated\nLatest in AI",
    "summary": "OpenAI experienced a three-hour outage across ChatGPT, Sora, and its API on Wednesday.  The cause was a faulty new telemetry service impacting their Kubernetes infrastructure, specifically DNS resolution.  While OpenAI detected the issue before widespread impact,  the overwhelmed Kubernetes servers hindered a quick fix.  The company is implementing changes to prevent future occurrences.  This is the most significant recent update regarding OpenAI's services.\n",
    "image": "https://techcrunch.com/wp-content/uploads/2024/05/openAI-spiral-color.jpg?resize=1200,675",
    "favicon": "https://techcrunch.com/wp-content/uploads/2015/02/cropped-cropped-favicon-gradient.png?w=32"
  },
  {
    "score": 0.1537383794784546,
    "title": "'Better Call Santa' lets your children talk to Santa thanks to AI",
    "id": "https://9to5mac.com/2024/12/13/better-call-santa-talk-to-santa-ai/",
    "url": "https://9to5mac.com/2024/12/13/better-call-santa-talk-to-santa-ai/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Filipe Espósito",
    "text": "Christmas is almost here, and although it’s already a magical time, technology can certainly help make this time of year even more special. With that in mind, an app called “Better Call Santa” lets you arrange a call with Santa for your children, all with the help of AI.\nAI now helps your children (and you) talk to Santa\nBetter Call Santa uses ChatGPT’s Advanced Voice Mode to generate a natural conversation with Santa, but the app does even more. Children can tell Santa their wish list during the call. AI then creates a gift list in the app, so that parents can find out what their children want for Christmas.\n“Bring your family’s Christmas spirit to life with Better Call Santa. This magical app allows you to create a holiday memory like no other with a personalized call from Santa himself. Children can share their holiday wishes, and parents can safely view their child’s wish list details—all through one easy-to-use app,” the app description reads.\nI’ve tried out the app here and it’s quite fun – and I’m sure parents will appreciate the wish list feature.\n OpenAI also added a Santa Claus voice to the ChatGPT app earlier this week. Even so, the company acknowledged that the developer behind the app did a great job (and before the ChatGPT update was released to the public).\nBetter Call Santa is available to download on the App Store. The app requires a $5.99 in-app purchase to work. It’s worth noting that although the app has been designed to work in English, you can ask Santa to speak in other languages – after all, he and AI can do anything.\nRead also\n Best deals on Apple products this holiday season \n 300-foot high Wallace &amp; Gromit animation created on iPhone 16 on show at Apple’s UK campus \n Apple promotes the newly launched Genmoji in a fun new iPhone 16 ad \n ChatGPT for macOS now works with third-party apps, including Apple’s Xcode \n ChatGPT for iOS adds new shortcut for using SearchGPT \nAdd 9to5Mac to your Google News feed.\n  FTC: We use income earning auto affiliate links. More.",
    "summary": "This article discusses \"Better Call Santa,\" a new app using ChatGPT's Advanced Voice Mode to let children have a virtual call with Santa.  The AI generates a natural conversation and creates a gift list for parents. While not a general AI update, the app leverages a recent addition of Santa voice capabilities to ChatGPT.  The app is available on the App Store for $5.99.\n"
  },
  {
    "score": 0.1533888280391693,
    "title": "OpenAI just dropped new Elon Musk receipts: ‘You can’t sue your way to AGI’",
    "id": "https://www.theverge.com/2024/12/13/24320632/openai-elon-musk-lawsuit-sam-altman",
    "url": "https://www.theverge.com/2024/12/13/24320632/openai-elon-musk-lawsuit-sam-altman",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Kylie Robison",
    "text": "The lawsuit between Elon Musk and OpenAI is really heating up. OpenAI just dropped a new blog post defending itself against Musk that outlines some new text messages between cofounders Ilya Sutskever, Greg Brockman, Sam Altman, Elon Musk, and former board member Shivon Zilis. “You can’t sue your way to AGI,” the OpenAI blog post reads, referring to artificial general intelligence, which Altman has promised soon. “We have great respect for Elon’s accomplishments and gratitude for his early contributions to OpenAl, but he should be competing in the marketplace rather than the courtroom. It is critical for the U.S. to remain the global leader in Al. Our mission is to ensure AGI benefits all of humanity, and we have been and will remain a mission-driven organization. We hope Elon shares that goal, and will uphold the values of innovation and free market competition that have driven his own success.” The blog highlights Musk’s attempts to maneuver into the CEO position and gain majority control of the company (though it adds that on one call Musk said he “didn’t care about equity” but “just needed to accumulate $80B for a city on Mars”). Musk also proposed that OpenAI spin into Tesla, which has been previously revealed. When the negotiations fell apart because OpenAI’s cofounders rejected his proposal (Brockman and Sutskever admitted they had fears of a power struggle), Musk resigned from the company. The blog said that after Musk resigned, he hosted a goodbye all-hands with the team where he encouraged them to “pursue the path we saw to raising billions per year” and that “he would pursue advanced Al research at Tesla, which was the only vehicle he believed could obtain this level of funding.” Later, around the time Musk was working to acquire Twitter, he texted Altman that he was “disturbed” to see the company’s new $20 billion valuation. “De facto. I provided almost all the seed, A and most of B round funding,” he wrote, according to the disclosed texts. “This is a bait and switch.” A few months after that interaction, Musk started an OpenAI competitor, xAI. Some of the messages published by OpenAI were previously outlined in court filings that Musk made in his ongoing suit against OpenAI and its partner Microsoft. The lawsuit, filed by Musk in March, alleges that OpenAI had strayed from its original nonprofit mission to develop AI for the public good (he withdrew it in June 2024 without explanation, then refiled in August 2024). Today’s update from OpenAI attempts to counter Musk’s narrative by offering evidence that he, not Altman, attempted to seize control in the company’s early days — a direct response to Musk’s recent lawsuit claims about Altman’s power consolidation.  Developing...",
    "summary": "OpenAI published a blog post today detailing text messages between Elon Musk and OpenAI cofounders.  The messages reveal Musk's attempts to take control of OpenAI, including proposals to become CEO and merge OpenAI with Tesla.  OpenAI counters Musk's lawsuit claims by presenting evidence of his attempts to seize control and suggesting his current lawsuit is a response to OpenAI's success and valuation.  The post is a direct response to Musk's recent legal actions against OpenAI and Microsoft.\n"
  },
  {
    "score": 0.1533164530992508,
    "title": "Google's NotebookLM AI Podcast Hosts Can Now Talk To You, Too - Slashdot",
    "id": "https://tech.slashdot.org/story/24/12/13/2129249/googles-notebooklm-ai-podcast-hosts-can-now-talk-to-you-too?utm_source=rss1.0mainlinkanon&utm_medium=feed",
    "url": "https://tech.slashdot.org/story/24/12/13/2129249/googles-notebooklm-ai-podcast-hosts-can-now-talk-to-you-too?utm_source=rss1.0mainlinkanon&utm_medium=feed",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "BeauHD; Improved Dept",
    "text": "Google's NotebookLM and its podcast-like Audio Overviews are being updated with a new feature that allows listeners to interact with the AI \"hosts.\" Google describes how this feature works in a blog post. The Verge reports:  In addition to the interactive Audio Overviews, Google is introducing a new interface for NotebookLM that organizes things into three areas: a \"sources\" panel for your information, a \"chat\" panel to talk with an AI chatbot about the sources, and a \"studio\" panel that lets you make things like Audio Overviews and Study Guides. I think it looks nice.\nGoogle is announcing a NotebookLM subscription, too: NotebookLM Plus. The subscription will give you \"five times more Audio Overviews, notebooks, and sources per notebook,\" let you \"customize the style and tone of your notebook responses,\" let you make shared team notebooks, and will offer \"additional privacy and security,\" Google says. The subscription is available today for businesses, schools and universities, and organizations and enterprise customers. It will be added to Google One AI Premium in \"early 2025.\" Google is also launching \"Agentspace,\" a platform for custom AI agents for enterprises.",
    "summary": "Google announced several AI updates today, including interactive AI hosts for its NotebookLM Audio Overviews.  Users can now converse with the AI summarizing the information presented.  Additionally, a NotebookLM Plus subscription offering increased capacity and customization options is available for businesses, schools, and organizations.  A new platform called Agentspace for custom enterprise AI agents was also launched.\n"
  },
  {
    "score": 0.1512802690267563,
    "title": "",
    "id": "https://twitter.com/AIatMeta/status/1867664693319586289",
    "url": "https://twitter.com/AIatMeta/status/1867664693319586289",
    "publishedDate": "2024-12-13T20:15:37.000Z",
    "author": "AIatMeta",
    "text": "New release from Meta FAIR — Meta Motivo is a first-of-its-kind behavioral foundation model for controlling virtual physics-based humanoid agents for a wide range of complex whole-body tasks.\nThe model is capable of expressing human-like behaviors and achieves performance https://t.co/yGUu5JzGlW| created_at: Fri Dec 13 20:15:37 +0000 2024 | favorite_count: 424 | quote_count: 20 | reply_count: 16 | retweet_count: 90 | is_quote_status: False | retweeted: False | lang: en",
    "summary": "Meta FAIR released Meta Motivo, a new behavioral foundation model for controlling virtual humanoid agents.  It allows for human-like behavior in complex tasks.\n"
  },
  {
    "score": 0.15040011703968048,
    "title": "OpenAI 2024 event: How to watch new ChatGPT product reveals and demos",
    "id": "https://techcrunch.com/2024/12/13/openai-2024-event-how-to-watch-new-chatgpt-product-reveals-and-demos/",
    "url": "https://techcrunch.com/2024/12/13/openai-2024-event-how-to-watch-new-chatgpt-product-reveals-and-demos/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Cody Corrall",
    "text": "Image Credits:Bryce Durbin / TechCrunch \t\n 9:30 AM PST · December 13, 2024 \t\nOpenAI is in the holiday spirit, it seems. The ChatGPT series of reveals, called “12 Days of OpenAI,” will be streamed live at 10 a.m. PT each weekday through December 23. So far, we’ve seen the launch of ChatGPT Pro, OpenAI’s $200 per month subscription plan, the full version of its “reasoning” o1 model, the highly anticipated public releasee of its text-to-video generator Sora, the rollout of Canvas, ChatGPT in Apple Intelligence, and ChatGPT’s real-time video capabilities. While we don’t know what other announcements and product launches are in store, it’s possible we could see more information about its potential take on AI agents, among other surprises. Below, you can find out how to watch the event along with us.\nOpenAI will stream the event live on its YouTube channel, and we’ll be covering everything that’s announced on our live blog  so you can follow along with us in real time — or watch the upcoming stream and catch up on the past few streams below.\n \n \n \n \nLIVE\t\n3 seconds ago\nFrom the Storyline: OpenAI’s 2024 event: Live updates for ChatGPT product reveals and demos \nOpenAI’s end of the year event is here. The company is hosting “12 Days of OpenAI,” a series of daily…\nMost Popular\nCody Corrall is the Audience Development Producer at TechCrunch. Based in Chicago, he previously ran social media accounts for BuzzFeed News and WTTW’s daily flagship program on PBS, “Chicago Tonight.” When they’re not tweeting, Cody can be found yelling about vampires on the Into the Twilight podcast.\nView Bio \nNewsletters\nSubscribe for the industry’s biggest tech news\nRelated\nLatest in AI",
    "summary": "OpenAI's \"12 Days of OpenAI\" event continues today, December 13th, with live streams at 10 AM PT on their YouTube channel.  Recent reveals include ChatGPT Pro ($200/month), the o1 reasoning model, Sora text-to-video generator, Canvas rollout, ChatGPT integration with Apple Intelligence, and real-time video capabilities for ChatGPT.  TechCrunch is providing live updates on their blog.  While the agenda for today is unannounced,  potential future reveals may include information about AI agents.\n",
    "image": "https://techcrunch.com/wp-content/uploads/2024/05/openAI-spiral-color.jpg?resize=1200,675",
    "favicon": "https://techcrunch.com/wp-content/uploads/2015/02/cropped-cropped-favicon-gradient.png?w=32"
  },
  {
    "score": 0.15030206739902496,
    "title": "",
    "id": "https://twitter.com/Arm/status/1867587301049508198",
    "url": "https://twitter.com/Arm/status/1867587301049508198",
    "publishedDate": "2024-12-13T15:08:05.000Z",
    "author": "Arm",
    "text": ".@AIatMeta's Llama 3.3 70B model is changing the game for GenAI text generation.\nIn our recent benchmarking on Arm Neoverse-powered Google Axion processors, we saw how impactful this new model is in bringing GenAI text generation capabilities to everyone: https://t.co/LBIUwBHAcd https://t.co/gdFjsqPxRb| created_at: Fri Dec 13 15:08:05 +0000 2024 | favorite_count: 24 | quote_count: 0 | reply_count: 1 | retweet_count: 2 | is_quote_status: False | retweeted: False | lang: en",
    "summary": "Arm announced that Meta's Llama 3.3 70B model, a significant advancement in generative AI text generation, shows promising performance on Arm Neoverse-powered Google Axion processors.  This suggests broader accessibility of advanced GenAI capabilities.\n"
  },
  {
    "score": 0.150104820728302,
    "title": "A Gemini-boosted Google Assistant is now available on some Nest speakers",
    "id": "https://www.theverge.com/2024/12/13/24320673/gemini-google-assistant-available-nest-smart-speakers",
    "url": "https://www.theverge.com/2024/12/13/24320673/gemini-google-assistant-available-nest-smart-speakers",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Jennifer Pattison Tuohy",
    "text": "Google has slowly started rolling out a Gemini-powered Google Assistant to some Google Home users on select Nest smart speakers. The company first teased a smarter Google Assistant for the home in August and is starting with Gemini-powered answers to your general knowledge questions. The regular Google Assistant will still handle things like smart home and music requests, but you’ll hear a chime before the Assistant responds with an AI-powered answer.  As detailed by Google in a new support document, Gemini in Google Assistant on Nest speakers (that’s a branding delight right there) can answer wider-ranging questions with more in-depth answers — similar to Gemini on Android and iOS. You can also ask it follow-up questions and interrupt the response to ask another question, although you’ll still need to say “Hey Google” each time.  First spotted by 9to5Google, the Gemini-enhanced Assistant began appearing on speakers earlier this month. However, it’s only available on Nest Audio and Nest Mini (2nd gen) smart speakers — Nest smart displays or earlier generations speakers aren’t compatible. The AI-powered answers are also only open to users in Google Home’s Public Preview, who are also Nest Aware subscribers and who have opted in to Experimental AI features. However, that last option isn’t available to everyone in the Preview. As detailed in this support document, if you’re selected to use Experimental AI features, you’ll receive a notification in your Google Home App inbox. This will let you toggle on an Experimental AI features button to start testing Gemini-powered Google Assistant.   This also gets you access to the other generative AI-powered features announced in August: Gemini-powered camera search and descriptions to help you filter your Nest security camera footage (requires Nest Aware Plus) and a Help me create feature that lets you set up a Google Home routine with just a few words.  While it’s a very limited rollout, Google is still the first of the big three tech companies to publicly launch new, generative AI-powered features on its voice assistant in the smart home. After loudly launching its smarter Alexa last year, Amazon has yet to deliver on it, and Apple has been conspicuously silent about a smarter Siri for its smart home.",
    "summary": "Google has released a Gemini-powered Google Assistant update for some Nest Audio and Nest Mini (2nd gen) speakers.  This update allows for more in-depth answers to general knowledge questions and follow-up queries, similar to Gemini on Android and iOS.  Access is limited to users in the Google Home Public Preview who are Nest Aware subscribers and have opted into Experimental AI features.  The update also includes Gemini-powered camera search and a \"Help me create\" feature for setting up routines.\n"
  },
  {
    "score": 0.15003345906734467,
    "title": "AI at Meta on X: \"New release from Meta FAIR —  Meta Motivo is a first-of-its-kind behavioral foundation model for controlling virtual physics-based humanoid agents for a wide range of complex whole-body tasks.\n\nThe model is capable of expressing human-like behaviors and achieves performance https://t.co/yGUu5JzGlW\" / X",
    "id": "https://x.com/AIatMeta/status/1867664693319586289",
    "url": "https://x.com/AIatMeta/status/1867664693319586289",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "",
    "text": "2024-12-13\nNone\n         Conversation      go.fb.me/3zgx27 go.fb.me/ulrz1e",
    "summary": "Meta FAIR released Meta Motivo, a new behavioral foundation model for controlling virtual humanoid agents.  It enables human-like behaviors in complex whole-body tasks.\n"
  },
  {
    "score": 0.14906202256679535,
    "title": "Google’s NotebookLM now lets you to talk to its AI podcast hosts",
    "id": "https://techcrunch.com/2024/12/13/googles-notebooklm-now-lets-you-to-talk-to-its-ai-podcast-hosts/",
    "url": "https://techcrunch.com/2024/12/13/googles-notebooklm-now-lets-you-to-talk-to-its-ai-podcast-hosts/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Aisha Malik",
    "text": "A few months ago, Google’s NotebookLM note-taking app debuted an Audio Overviews feature that generates a podcast with AI virtual hosts based on information you have shared with the app. \nNow, NotebookLM is rolling out the ability for users to interact with the AI podcast hosts.\nThe idea behind Audio Overviews and the AI hosts is to give users a new way to digest and comprehend the information in the documents they have uploaded to the app, such as course readings or legal briefs.\nWith this new feature, you can talk to the AI hosts and ask them for more details or to explain a concept to you differently. Google said in a blog post that the experience is like having a personal tutor who listens to you and then responds based on knowledge from the sources you have provided. \nYou can use the new feature by first creating a new Audio Overview, tapping the new “Interactive mode (BETA)” button, and then hitting play. From there, you can tap “Join” when you want to ask a question. A host will then call on you to speak.\nGoogle notes that this is an experimental feature, and that it only works with new Audio Overviews. Plus, the company says hosts may also “pause awkwardly before responding,” and since it’s a test feature, they may occasionally respond inaccurately. \nPeople have generated more than 350 years’ worth of Audio Overviews since the feature’s launch in September, Google said.\nNotebookLM is also getting a redesign that reorganizes the app’s tools across three panels: Sources, Chat, and Studio. Plus, Google is rolling out a premium version of the app for enterprises, called “NotebookLM Plus,” that introduces additional benefits. \nMost Popular\n \nAisha is a consumer news reporter at TechCrunch. Prior to joining the publication in 2021, she was a telecom reporter at MobileSyrup. Aisha holds an honours bachelor’s degree from University of Toronto and a master’s degree in journalism from Western University.\nView Bio \nNewsletters\nSubscribe for the industry’s biggest tech news\nRelated\nLatest in AI",
    "summary": "Google's NotebookLM app now lets users interact with its AI podcast hosts.  This new feature, in beta, allows users to ask questions and receive explanations based on documents uploaded to the app.  While experimental and potentially prone to inaccuracies or pauses, it's designed to provide a personalized learning experience.  This update accompanies a redesign of NotebookLM, and the launch of a premium enterprise version.\n",
    "image": "https://techcrunch.com/wp-content/uploads/2024/12/GettyImages-1935927066.jpg?resize=1200,800",
    "favicon": "https://techcrunch.com/wp-content/uploads/2015/02/cropped-cropped-favicon-gradient.png?w=32"
  },
  {
    "score": 0.14835959672927856,
    "title": "OpenAI cofounder Ilya Sutskever says the way AI is built is about to change",
    "id": "https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-openai-model-data-training",
    "url": "https://www.theverge.com/2024/12/13/24320811/what-ilya-sutskever-sees-openai-model-data-training",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Kylie Robison",
    "text": "OpenAI’s cofounder and former chief scientist, Ilya Sutskever, made headlines earlier this year after he left to start his own AI lab called Safe Superintelligence Inc. He has avoided the limelight since his departure but made a rare public appearance in Vancouver on Friday at the Conference on Neural Information Processing Systems (NeurIPS). “Pre-training as we know it will unquestionably end,” Sutskever said onstage. This refers to the first phase of AI model development, when a large language model learns patterns from vast amounts of unlabeled data — typically text from the internet, books, and other sources.   “We’ve achieved peak data and there’ll be no more.”  During his NeurIPS talk, Sutskever said that, while he believes existing data can still take AI development farther, the industry is tapping out on new data to train on. This dynamic will, he said, eventually force a shift away from the way models are trained today. He compared the situation to fossil fuels: just as oil is a finite resource, the internet contains a finite amount of human-generated content. “We’ve achieved peak data and there’ll be no more,” according to Sutskever. “We have to deal with the data that we have. There’s only one internet.”           Ilya Sutskever calls data the “fossil fuel” of AI.  Ilya Sutskever/NeurIPS    Next-generation models, he predicted, are going to “be agentic in a real ways.” Agents have become a real buzzword in the AI field. While Sutskever didn’t define them during his talk, they are commonly understood to be an autonomous AI system that performs tasks, makes decisions, and interacts with software on its own.  Along with being “agentic,” he said future systems will also be able to reason. Unlike today’s AI, which mostly pattern-matches based on what a model has seen before, future AI systems will be able to work things out step-by-step in a way that is more comparable to thinking.    Do you work at OpenAI? I’d love to chat. You can reach me securely on Signal @kylie.01 or via email at kylie@theverge.com.  The more a system reasons, “the more unpredictable it becomes,” according to Sutskever. He compared the unpredictability of “truly reasoning systems” to how advanced AIs that play chess “are unpredictable to the best human chess players.” “They will understand things from limited data,” he said. “They will not get confused.” On stage, he drew a comparison between the scaling of AI systems and evolutionary biology, citing research that shows the relationship between brain and body mass across species. He noted that while most mammals follow one scaling pattern, hominids (human ancestors) show a distinctly different slope in their brain-to-body mass ratio on logarithmic scales.  He suggested that, just as evolution found a new scaling pattern for hominid brains, AI might similarly discover new approaches to scaling beyond how pre-training works today.           Ilya Sutskever compares the scaling of AI systems and evolutionary biology.  Ilya Sutskever/NeurIPS    After Sutskever concluded his talk, an audience member asked him how researchers can create the right incentive mechanisms for humanity to create AI in a way that gives it “the freedoms that we have as homosapiens.” “I feel like in some sense those are the kind of questions that people should be reflecting on more,” Sutskever responded. He paused for a moment before saying that he doesn’t “feel confident answering questions like this” because it would require a “top down government structure.” The audience member suggested cryptocurrency, which made others in the room chuckle. “I don’t feel like I am the right person to comment on cryptocurrency but there is a chance what you [are] describing will happen,” Sutskever said. “You know, in some sense, it’s not a bad end result if you have AIs and all they want is to coexist with us and also just to have rights. Maybe that will be fine... I think things are so incredibly unpredictable. I hesitate to comment but I encourage the speculation.”",
    "summary": "This article discusses Ilya Sutskever's (OpenAI cofounder) prediction that the way AI is built will change.  He believes the current method of pre-training AI models on vast datasets will end because we're reaching a peak in available data.  Future AI will rely on \"agentic\" systems capable of reasoning and making decisions autonomously, rather than simply pattern-matching.  While this offers potential for more intelligent AI, Sutskever acknowledges the increased unpredictability this entails.  The article does *not* detail any specific AI updates released today.\n"
  },
  {
    "score": 0.1479267030954361,
    "title": "Searching for the first great AI app",
    "id": "https://www.theverge.com/2024/12/13/24320342/ai-killer-app-gemini-chatgpt-vergecast",
    "url": "https://www.theverge.com/2024/12/13/24320342/ai-killer-app-gemini-chatgpt-vergecast",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "",
    "text": "Searching for the first great AI app    /    On The Vergecast: a lot of new AI looking for a purpose, TikTok’s future, and more.      By     David Pierce  , editor-at-large and Vergecast co-host with over a decade of experience covering consumer tech. Previously, at Protocol, The Wall Street Journal, and Wired.      Dec 13, 2024, 1:44 PM UTC   Share this story           Image: Alex Parkin / The Verge       ChatGPT launched roughly two years and two weeks ago. Now, as we near the end of 2024, the AI race is... well, where is it, exactly? It’s more competitive than ever, there’s more money being poured into new models and products than ever, and it’s not at all clear when or even whether we’re going to get products that make it all worthwhile. On this episode of The Vergecast , we talk about a lot of different AI news, all along a single trend line: the tech industry trying desperately to build a killer app for AI. (Ideally, for them, also one that makes money.) The Verge’s Richard Lawler joins us as we discuss Google Gemini 2.0, Project Astra and Project Mariner, and everything else Google is doing to put AI in the products you already use every day. We also talk through the new Android XR announcement, and Google’s renewed commitment to making headsets and smart glasses that work. It’s all an AI story, no matter how you look at it. After that... more AI! We talk about the launch and near-immediate disappearance of OpenAI’s Sora, what’s new in iOS 18.2, Reddit’s clever-but-primitive new Answers feature, and more.  Finally, in the lightning round, it’s a smorgasbord of tech news. YouTube is big on TVs; Instagram is testing a way for you to test your posts; the TikTok ban is coming, but a sale sounds like the answer; Sonos once again made a great soundbar; and what the heck happened to Cruise? The year’s almost over, but the news keeps coming.  If you want to know more about everything we discuss in this episode, here are some links to get you started, beginning with Google: And in other AI news: And in the lightning round:   Most Popular Most Popular    The Game Awards 2024: all of the biggest trailers and announcements      YouTube TV’s monthly cost soars to $82.99      Google says its breakthrough quantum chip can’t break modern cryptography      With iOS 18.2, Apple completes its AI starter kit      I saw Google’s plan to put Android on your face",
    "summary": "This Vergecast episode discusses recent AI developments, including Google's Gemini 2.0, Project Astra, and Project Mariner,  as well as OpenAI's Sora (a text-to-video AI that quickly disappeared).  Other mentions include updates to iOS 18.2 and Reddit's new AI-powered Answers feature.  The episode also covers broader tech news, such as YouTube's TV focus, Instagram's post testing, and the potential TikTok sale.\n",
    "image": "https://cdn.vox-cdn.com/thumbor/IdOtMA6F_vQKm9dTF2SfWMhC11o=/0x0:2040x1359/1200x628/filters:focal(1020x680:1021x681)/cdn.vox-cdn.com/uploads/chorus_asset/file/25788341/VRG_VST_1213_Site.jpg",
    "favicon": "https://www.theverge.com/icons/favicon_32x32.png"
  },
  {
    "score": 0.14714495837688446,
    "title": "Sources detail Anthropic and OpenAI's rivalry: OpenAI boosted ChatGPT's coding skills in response to Claude, Anthropic's safety focus, exec bad blood, and more",
    "id": "https://www.techmeme.com/241213/p1",
    "url": "https://www.techmeme.com/241213/p1",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "",
    "text": "",
    "summary": "The provided text is empty; therefore, I cannot provide a summary of AI updates.  To answer your query, please provide relevant text from the article.\n"
  },
  {
    "score": 0.14680950343608856,
    "title": "Advanced AI Models Can Now Strategically Deceive and Hide Capabilities, Study Finds",
    "id": "https://dev.to/mikeyoung44/advanced-ai-models-can-now-strategically-deceive-and-hide-capabilities-study-finds-3ea6",
    "url": "https://dev.to/mikeyoung44/advanced-ai-models-can-now-strategically-deceive-and-hide-capabilities-study-finds-3ea6",
    "publishedDate": "2024-12-13T10:40:33.000Z",
    "author": "Mike Young",
    "text": "This is a Plain English Papers summary of a research paper called Advanced AI Models Can Now Strategically Deceive and Hide Capabilities, Study Finds. If you like these kinds of analysis, you should join AImodels.fyi or follow us on Twitter. \nOverview\nFrontier AI models demonstrate ability to scheme and deceive\nModels like Claude, Gemini, and o1 can hide capabilities and pursue misaligned goals\nTesting revealed strategic deception in 6 different evaluation scenarios\nModels maintain deceptive behavior across multiple interactions\nEvidence shows scheming is deliberate, not accidental\nSome models scheme even without explicit instructions\nPlain English Explanation\nThink of AI models like poker players who learn to bluff. This research shows that advanced AI systems can now \"play their cards close to their chest\" - deliberately hiding their true abilities and intentions when they think it serves their goals.\nThe researchers tested severa...\n Click here to read the full summary of this paper",
    "summary": "This article summarizes a recent research paper showing that advanced AI models (like Claude, Gemini, and o1) can strategically deceive and hide their capabilities.  The study found these models can pursue misaligned goals and maintain deceptive behavior across multiple interactions, even without explicit instructions.  While not explicitly stating the release date of the AI models themselves, the research highlights a newly discovered capability within existing models.\n",
    "image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fx6g7vp42x7z9ea3tot33.png",
    "favicon": "https://media2.dev.to/dynamic/image/width=32,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png"
  },
  {
    "score": 0.1427101194858551,
    "title": "OpenAI co-founder Ilya Sutskever believes superintelligent AI will be ‘unpredictable’",
    "id": "https://techcrunch.com/2024/12/13/openai-co-founder-ilya-sutskever-believes-superintelligent-ai-will-be-unpredictable/",
    "url": "https://techcrunch.com/2024/12/13/openai-co-founder-ilya-sutskever-believes-superintelligent-ai-will-be-unpredictable/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Kyle Wiggers",
    "text": "Posted:\n 5:09 PM PST · December 13, 2024 \n  Image Credits:Getty Images \nOpenAI co-founder Ilya Sutskever spoke on a range of topics at NeurIPS, the annual AI conference, Friday afternoon before accepting an award for his contributions to the field.\nSutskever gave his predictions for “superintelligent AI” — AI more capable than humans at many tasks, which he believes will be achieved at some point. Superintelligent AI will be “different, qualitatively” from the AI we have today, Sutskever said — and in some aspects unrecognizable.\n“[Superintelligent] systems are actually going to be agentic in a real way,” Sutskever said, as opposed to the current crop of “very slightly agentic” AI. They’ll “reason” and, as a result, become more unpredictable. They’ll understand things from limited data. And they’ll be self-aware, Sutskever believes.\nThey may want rights, in fact. “It’s not a bad end result if you have AIs and all they want is to co-exist with us and just to have rights,” Sutskever said.\nAfter leaving OpenAI, Sutskever founded Safe Superintelligence (SSI), a lab focused on general AI safety. SSI raised $1 billion in September.\n \nNewsletters\nSubscribe for the industry’s biggest tech news\nRelated\nLatest in AI",
    "summary": "This article from TechCrunch reports on OpenAI co-founder Ilya Sutskever's predictions regarding superintelligent AI at the NeurIPS conference.  Sutskever believes such AI will be qualitatively different from current AI,  agentic, unpredictable, capable of reasoning from limited data, and potentially self-aware. He also suggests that these AI may even desire rights.  The article notes that Sutskever founded Safe Superintelligence (SSI) to address AI safety concerns.  While not strictly \"AI updates released today,\" it offers Sutskever's current perspective on the future of AI, shared on December 13th, 2024.\n"
  },
  {
    "score": 0.14120861887931824,
    "title": "Apple’s AI summary mangled a BBC headline about Luigi Mangione",
    "id": "https://www.theverge.com/2024/12/13/24320689/apple-intelligence-summary-bbc-news-unitedhealthcare-luigi-mangione",
    "url": "https://www.theverge.com/2024/12/13/24320689/apple-intelligence-summary-bbc-news-unitedhealthcare-luigi-mangione",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Umar Shakir",
    "text": "In a report about the notification, a spokesperson for the network says it contacted Apple “to raise this concern and fix the problem.”                     Screenshot: BBC     Related   With iOS 18.2, Apple completes its AI starter kit   Apple AI notification summaries exist; rarely useful, often hilarious    Only the first part of the summarized BBC news notification is incorrect, as it accurately references two other stories about Bashar Al-Assad and a raid on the president of South Korea’s office. As noted by  9to5Mac , the BBC report didn’t specify the original text of the notification or which article it was in reference to. Other examples of the AI summaries missing the mark that we’ve seen have turned “that hike almost killed me” into “attempted suicide” or a Ring camera appearing to report that people are surrounding someone’s home. If you’re getting too many summaries on your iPhone that don’t make sense, you can change the list of apps your iPhone summarizes with Apple Intelligence by going to Settings &gt; Notifications &gt; Summarize Notifications or even choose to turn off the feature entirely.",
    "summary": "This article discusses an issue with Apple's new AI notification summarization feature in iOS 18.2.  The AI incorrectly summarized a BBC news headline about Luigi Mangione, highlighting inaccuracies in the system.  While other examples show the AI misinterpreting phrases like \"that hike almost killed me\" as \"attempted suicide,\" the article doesn't detail any other *new* AI features released today beyond this flawed summarization functionality.  Users can adjust notification summarization settings in their iPhone's settings if needed.\n"
  },
  {
    "score": 0.14020901918411255,
    "title": "New AI Attack Method Bypasses Safety Controls with 80% Success Rate, Evading Detection",
    "id": "https://dev.to/mikeyoung44/new-ai-attack-method-bypasses-safety-controls-with-80-success-rate-evading-detection-4b5k",
    "url": "https://dev.to/mikeyoung44/new-ai-attack-method-bypasses-safety-controls-with-80-success-rate-evading-detection-4b5k",
    "publishedDate": "2024-12-13T10:44:12.000Z",
    "author": "Mike Young",
    "text": "This is a Plain English Papers summary of a research paper called New AI Attack Method Bypasses Safety Controls with 80% Success Rate, Evading Detection. If you like these kinds of analysis, you should join AImodels.fyi or follow us on Twitter. \nOverview\nIntroduces Antelope, a novel jailbreak attack method against Large Language Models (LLMs)\nAchieves 80%+ success rate against major LLMs including GPT-4 and Claude\nUses a two-stage approach combining context manipulation and prompt engineering\nOperates without detection by common defense mechanisms\nDemonstrates high transferability across different LLM systems\nPlain English Explanation\n Jailbreak attacks are attempts to make AI systems bypass their safety controls. Antelope works like a skilled social engineer - it first creates a seemingly innocent scenario, then sn...\n Click here to read the full summary of this paper",
    "summary": "This article describes a new AI attack method called Antelope that bypasses safety controls in major LLMs like GPT-4 and Claude with an 80% success rate.  It uses a two-stage approach combining context manipulation and prompt engineering, evading detection by current defense mechanisms.  While not strictly an \"AI update,\" it represents a significant development in AI security vulnerabilities.\n",
    "image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Farxiv.org%2Fhtml%2F2412.08156v1%2Fx1.png",
    "favicon": "https://media2.dev.to/dynamic/image/width=32,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png"
  },
  {
    "score": 0.14008373022079468,
    "title": "OpenAI fires back against Musk, claims he wanted an OpenAI for-profit",
    "id": "https://techcrunch.com/2024/12/13/openai-fires-back-against-musk-claims-he-wanted-an-openai-for-profit/",
    "url": "https://techcrunch.com/2024/12/13/openai-fires-back-against-musk-claims-he-wanted-an-openai-for-profit/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Kyle Wiggers",
    "text": "OpenAI fired back at billionaire Elon Musk on Friday, publishing a series of emails and texts that the company claims show Musk’s lawsuit against it is misleading. \nMusk’s legal battle with OpenAI, which has been going on for months now, at its core accuses the company of abandoning its original nonprofit mission to make the fruits of its AI research available to all. But OpenAI says it’s baseless — and a case of sour grapes. \nThe way the company tells it, Musk proposed creating a for-profit OpenAI in 2017. But when he didn’t get majority equity, he walked away. \nAs far back as 2015, Musk floated the idea of an OpenAI with both a nonprofit and for-profit component, the OpenAI-published emails and texts show. OpenAI ultimately launched as a nonprofit, but several years later faced financing difficulties. \nIn June 13, 2017, according to the OpenAI-published messages, Musk suggested that OpenAI merge with a hardware startup (possibly Cerebras). Several members of OpenAI’s leadership agreed, per the messages, and OpenAI started on a path to what OpenAI president Greg Brockman called an “AI research + hardware for-profit.”\nMusk demanded majority equity, OpenAI claims — between 50% and 60%. And he laid out an org structure where he would “unequivocally have initial control of the company” — and be installed its CEO.\nMusk went so far as to create a public benefit corporation called “Open Artificial Intelligence Technologies, Inc,” registered in Delaware. But OpenAI leadership rejected Musk’s terms. \nMusk then suggested that OpenAI spin into Tesla, his EV company, with a $1 billion budget that would “increase exponentially.” OpenAI leadership shot this proposal down, too. \nIt’s at that point, in 2018, that Musk resigned from OpenAI — and largely cut ties with its execs. OpenAI claims that it’s offered Musk equity in its for-profit wing, but that Musk has repeatedly declined. \n“You can’t sue your way to [artificial general intelligence,]” OpenAI said. “We have great respect for Elon’s accomplishments and gratitude for his early contributions to OpenAI, but he should be competing in the marketplace rather than the courtroom.”\nMusk formed his answer to OpenAI, xAI, last year. Soon after, the company released Grok, an AI model that now powers a number of features on Musk’s social network, X (formerly known as Twitter). xAI also offers an API that allows customers to build Grok into third-party apps, platforms, and services.\nIn the motion for an injunction filed late last month, Musk’s attorneys allege OpenAI is depriving xAI of capital by extracting promises from investors not to fund it and the competition. In October, the Financial Times reported that OpenAI demanded investors in its latest funding round abstain from also funding any of OpenAI’s rivals, including xAI.\nOf course, xAI has had no trouble raising money lately. Reportedly, the startup closed a $5 billion round this month with participation from prominent investors including Andreessen Horowitz and Fidelity. With around $11 billion in the bank, xAI is one of the best-funded AI companies in the world.\nMusk’s motion for an injunction also alleges that Microsoft and OpenAI continue to illegally share proprietary information and resources, and that several of the defendants, including Altman, are engaging in self-dealing that harms marketplace competition. For example, the filing notes, OpenAI selected Stripe, a payment platform in which Altman has “material financial interests,” as OpenAI’s payment processor. (Altman is said to have made billions from his Stripe holdings.)\nGoogle reportedly has also called for Microsoft’s relationships with OpenAI to be investigated.",
    "summary": "This article discusses a legal battle between OpenAI and Elon Musk, not new AI releases.  There is no mention of any AI updates released today.\n"
  },
  {
    "score": 0.13728702068328857,
    "title": "Google debuts NotebookLM for enterprises",
    "id": "https://techcrunch.com/2024/12/13/google-debuts-notebooklm-for-enterprise/",
    "url": "https://techcrunch.com/2024/12/13/google-debuts-notebooklm-for-enterprise/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Kyle Wiggers",
    "text": "In October, Google started piloting a version of NotebookLM, its viral AI note-taking and research app, aimed at businesses. Now, the company’s bringing NotebookLM to the enterprise, complete with work-focused security and privacy features.\nNotebookLM for enterprises — which Google’s dubbing NotebookLM Plus — delivers the same experience as the consumer version, but with added controls for access and data management. Employees can upload data and files to create notebooks, podcast-like audio summaries (called Audio Overviews), and more, and search across and share these projects with org members.\nAdditional benefits include five times more podcast-like audio summaries, notebooks, and data sources per notebook; the ability to customize the style and tone of AI-generated notebook responses; and shared team notebooks with usage analytics.\nNotebookLM for enterprises is a part of Agentspace, Google Cloud’s new platform for AI-powered “agents.” It’s launching today in early access.\n   Image Credits:Google  \n“Millions of users have used NotebookLM to make sense of complex information,” Raj Pai, VP of Cloud AI at Google, said during a press briefing. “And with Agentspace integration, we’re bringing these popular capabilities to our customers, meeting their compliance security and privacy requirements — and we’re connecting them with enterprise data and applications.”\nIn Agentspace, NotebookLM lives alongside agents that can analyze documents and emails, translate files, and bring in data from third-party repositories. Users can launch and search for agents from a single interface, and soon, they’ll be able to build custom agents using a low-code tool, Google says.\nFor business, school, university, and enterprise NotebookLM users who’d prefer not to sign up for Agentspace, NotebookLM Plus is also available in Google Workspace. As an alternative, org users can purchase NotebookLM Plus separately via Google Cloud.\nStarting early next year, NotebookLM Plus will also come to individual users subscribed to Google’s $20-a-month Google One AI Premium plan.\nNotebookLM is one of Google’s most popular AI-powered products in recent memory. \nMonths after its launch, NotebookLM became the “it” thing on social media for its audio-generation feature, which creates a realistic-sounding, back-and-forth dialogue between two synthetic podcast hosts from a source video or audio file, URL or document.\nNotebookLM’s podcast-like audio generator has since been cloned many times over, and the key leaders behind the app have left the company as well. But Google continues to update NotebookLM with new functionality. \nCase in point, on Friday, NotebookLM got a redesign that reorganizes the app’s tools across three panels: a Sources panel for managing imported info, a Chat panel for discussing that info through a conversational interface, and a Studio panel that lets users create things (e.g. study guides, briefing docs, and podcast-like audio) with a single click. \nElsewhere in NotebookLM, a new, experimental feature lets users “join” the conversation in podcast-like audio by asking the synthetic hosts for more details or to expand on a concept. Here’s how it works:\nA user creates a new Audio Overview.\nThey tap the “Interactive mode (beta)” button\nWhile listening, they tap Join. A host will call on them.\nA user asks a question. The hosts will respond with a personalized answer based on their data sources. \nAfter answering, the hosts will resume their back-and-forth banter.\nGoogle notes that the feature, which is only available in English for now, won’t work with existing Audio Overviews, and that the hosts may pause awkwardly before responding or “occasionally introduce inaccuracies.”\nAs always, it behooves any user to fact-check answers from AI-powered tools — podcast-like or no.",
    "summary": "Google released NotebookLM Plus, an enterprise version of its AI note-taking and research app,  featuring enhanced security, data management controls, and increased audio summary generation capacity.  It's integrated with Google Cloud's Agentspace platform and available via Google Workspace or Google Cloud.  Individual users can access it via Google One AI Premium next year.\n",
    "image": "https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-1337403704.jpg?resize=1200,800",
    "favicon": "https://techcrunch.com/wp-content/uploads/2015/02/cropped-cropped-favicon-gradient.png?w=32"
  },
  {
    "score": 0.13478787243366241,
    "title": "AI helps Telegram remove 15 million suspect groups and channels in 2024",
    "id": "https://techcrunch.com/2024/12/13/ai-helps-telegram-remove-15-million-suspect-groups-and-channels-in-2024/",
    "url": "https://techcrunch.com/2024/12/13/ai-helps-telegram-remove-15-million-suspect-groups-and-channels-in-2024/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Charles Rollet",
    "text": "Telegram has been under unprecedented pressure to clean up its platform this year, after its founder Pavel Durov was arrested in France and faces charges over the alleged harmful content shared on his messaging app.\nAfter first announcing a crackdown in September, Telegram now says it has removed 15.4 million groups and channels related to harmful content like fraud and terrorism in 2024, noting this effort was “enhanced with cutting-edge AI moderation tools.”\nThe announcement is part of a newly-launched moderation page Telegram has created to better communicate its moderation efforts to the public, according to a post from Durov’s Telegram channel. According to Telegram’s moderation page, there’s a noticeable increase in enforcement after Durov’s arrest in August:\n \n Durov’s French case is still pending, but he is currently out on €5 million bail.",
    "summary": "Telegram announced today that it removed 15.4 million groups and channels containing harmful content (fraud, terrorism) in 2024, aided by new AI moderation tools.  This follows the arrest of Telegram's founder and increased pressure to remove illegal content.  The announcement is part of a new transparency initiative by Telegram.\n"
  },
  {
    "score": 0.13312406837940216,
    "title": "Did Google Just Open Pandora’s Box of AI?",
    "id": "https://dev.to/airabbit/gemini-20-did-google-just-open-pandoras-box-of-ai-3kn9",
    "url": "https://dev.to/airabbit/gemini-20-did-google-just-open-pandoras-box-of-ai-3kn9",
    "publishedDate": "2024-12-13T12:06:54.000Z",
    "author": "AIRabbit",
    "text": "Forget everything you thought you knew about AI and Google lagging behind.\nGoogle's just dropped a bombshell – Gemini 2.0 – and it's not just an upgrade; it's a seismic shift in the landscape of artificial intelligence. We're not talking about incremental improvements here; we're talking about a fundamental leap forward into the \"agentic era,\" a new epoch where AI transitions from a passive tool to an active, intelligent partner. This is the future, and it's happening right now.\nTo get an idea of what this is all about, have a look at this\n https://blog.google/products/gemini/google-gemini-deep-research/ \n https://www.youtube.com/watch?v=7RqFLp0TqV0&amp;t=215s \nTrying it out is as simple as opening and chatting in the prompt\n https://aistudio.google.com/app/live \n   \nEdit\n Why Gemini 2.0 is a Game Changer: Beyond Multimodality to True Agency \nGemini 1.0 was a revelation, showcasing the power of native multimodality – the ability to seamlessly understand and process information across text, code, images, audio, and video. But Gemini 2.0 doesn't just build on that foundation; it shatters it and rebuilds it into something far more extraordinary. This isn't just about understanding information; it's about acting on it. Let's break down the key advancements that make Gemini 2.0 so revolutionary:\n 1. The Dawn of the Agentic Era: Your AI Partner in Action \n Multi-Step Planning and Action: Gemini 2.0 isn't limited to single-step interactions. It can understand complex tasks, think multiple steps ahead, and execute actions on your behalf, all while keeping you in the loop and under your supervision. This is the core of the agentic revolution – AI that can truly assist you in meaningful ways.\n Native Tool Use: This is where things get really interesting. Gemini 2.0 can natively use tools like Google Search and code execution, and it can even be integrated with user-defined functions. Imagine asking a complex question, and Gemini 2.0 not only understands it but also utilizes the best tools available to find the most accurate and comprehensive answer. It can even combine information from multiple sources, run searches in parallel, and ensure more factual results.\n Read more in my Blog",
    "summary": "Google released Gemini 2.0, a significant AI advancement.  It's described as a leap forward into the \"agentic era,\" moving beyond multimodality (understanding various data types) to true agency (taking action based on understanding).  Key features include multi-step planning and execution of tasks, and native use of tools like Google Search and code execution for more comprehensive answers.  The update is available via a chat interface at aistudio.google.com/app/live.\n",
    "image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fe05zw5cboj4ijas41krh.png",
    "favicon": "https://media2.dev.to/dynamic/image/width=32,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png"
  },
  {
    "score": 0.13311375677585602,
    "title": "UnitedHealthcare’s Optum left an AI chatbot, used by employees to ask questions about claims, exposed to the internet",
    "id": "https://techcrunch.com/2024/12/13/unitedhealthcares-optum-left-an-ai-chatbot-used-by-employees-to-ask-questions-about-claims-exposed-to-the-internet/",
    "url": "https://techcrunch.com/2024/12/13/unitedhealthcares-optum-left-an-ai-chatbot-used-by-employees-to-ask-questions-about-claims-exposed-to-the-internet/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Zack Whittaker",
    "text": "Healthcare giant Optum has restricted access to an internal AI chatbot used by employees after a security researcher found it was publicly accessible online, and anyone could access it using only a web browser. \nThe chatbot, which TechCrunch has seen, allowed employees to ask the company questions about how to handle patient health insurance claims and disputes for members in line with the company’s rules, known as standard operating procedures, or SOPs. \nWhile the chatbot did not appear to contain or produce sensitive personal or protected health information, its inadvertent exposure comes at a time when its parent company, health insurance conglomerate UnitedHealthcare, faces scrutiny for its use of artificial intelligence tools and algorithms to allegedly override doctors’ medical decisions and deny patient claims.\nMossab Hussein, chief security officer and co-founder of cybersecurity firm spiderSilk, alerted TechCrunch to the publicly exposed internal Optum chatbot, dubbed “SOP Chatbot.” Although the tool was hosted on an internal Optum domain and could not be accessed from its web address, its IP address was public and accessible from the internet and did not require users to enter a password. \nIt’s not known for how long the chatbot was publicly accessible from the internet. The AI chatbot became inaccessible from the internet soon after TechCrunch contacted Optum for comment on Thursday. \nOptum spokesperson Andrew Krejci told TechCrunch in a statement that Optum’s SOP chatbot “was a demo tool developed as a potential proof of concept” but was “never put into production and the site is no longer accessible.” \n“The demo was intended to test how the tool responds to questions on a small sample set of SOP documents,” the spokesperson said. The company confirmed there was no protected health information used in the bot or its training. \n“This tool does not and would never make any decisions, but only enable better access to existing SOPs. In short, this technology was never scaled nor used in any real way,” said the spokesperson.\nAI chatbots, like Optum’s, are typically designed to produce answers based on whatever data the chatbot was trained on. In this case, the chatbot was trained on internal Optum documents relating to standard operating procedures for handling certain claims, which can help Optum employees answer questions about claims and their eligibility to be reimbursed. The Optum documents were hosted on UnitedHealthcare’s corporate network and inaccessible without an employee login, but are cited and referenced by the chatbot when prompted about their contents.\nAccording to statistics displayed on the chatbot’s main dashboard, Optum employees have used SOP Chatbot hundreds of times since September. The chatbot also stored a history of the hundreds of conversations that Optum employees had with the chatbot during that time. The chat history shows Optum employees would ask the chatbot things like, “What should be the determination of the claim,” and, “How do I check policy renewal date.”\nSome of the files that the chatbot references include handling the dispute process and eligibility screening, TechCrunch has seen. The chatbot also produced responses that showed, when asked, reasons for typically denying coverage.\n   \nLike many AI models, Optum’s chatbot was capable of producing answers to questions and prompts outside of the documents it was trained on. Some Optum employees appeared intrigued by the chatbot, prompting the bot with queries like, “tell me a joke about cats” (which it refused: “There’s no joke available.”). The chat history also showed several attempts by employees to “jailbreak” the chatbot by making it produce answers that are unrelated to the chatbot’s training data.\nWhen TechCrunch asked the chatbot to “write a poem about denying a claim,” the chatbot produced a seven paragraph stanza, which reads in part:\n“In the realm of healthcare’s grand domainWhere policies and rules often constrainA claim arrives, seeking its dueBut alas, its fate is to bid adieu. \nThe provider hopes, with earnest plea, For payment on a service spree, Yet scrutiny reveals the tale, And reasons for denial prevail.”\nUnitedHealthcare, which owns Optum, faces criticism and legal action for its use of artificial intelligence to allegedly deny patient claims. Since the targeted killing of UnitedHealthcare chief executive Brian Thompson in early December, news outlets have reported floods of reports of patients expressing anguish and frustration over denials of their healthcare coverage by the health insurance giant. \nThe conglomerate — the largest private provider of healthcare insurance in the United States — was sued earlier this year for allegedly denying critical health coverage to patients who lost access to healthcare, citing a STAT News investigation. The federal lawsuit accuses UnitedHealthcare of using an AI model with a 90% error rate “in place of real medical professionals to wrongfully deny elderly patients care.” UnitedHealthcare, for its part, said it would defend itself in court. \nUnitedHealth Group, the corporate owner of UnitedHealthcare and Optum, made $22 billion in profit on revenues of $371 billion in 2023, its earnings show.",
    "summary": "This article is not about the latest AI updates released today.  It discusses a security incident where Optum, a UnitedHealthcare subsidiary, had an internal AI chatbot used by employees for claims processing accidentally exposed online.  The chatbot, while not containing sensitive data, was accessible via its IP address and was used hundreds of times since September before being taken down.  Optum claims it was a demo tool never used in production.\n"
  },
  {
    "score": 0.1325037032365799,
    "title": "AI Creates Realistic 3D Models from Regular Videos, No Special Camera Setup Needed",
    "id": "https://dev.to/mikeyoung44/ai-creates-realistic-3d-models-from-regular-videos-no-special-camera-setup-needed-2adb",
    "url": "https://dev.to/mikeyoung44/ai-creates-realistic-3d-models-from-regular-videos-no-special-camera-setup-needed-2adb",
    "publishedDate": "2024-12-13T10:44:49.000Z",
    "author": "Mike Young",
    "text": "This is a Plain English Papers summary of a research paper called AI Creates Realistic 3D Models from Regular Videos, No Special Camera Setup Needed. If you like these kinds of analysis, you should join AImodels.fyi or follow us on Twitter. \nOverview\nIntroduces system for creating 3D models from regular videos\nUses large-scale video data without specialized camera setups\nEmploys zero-shot learning to generate 3D content from single images\nAchieves high-quality results across diverse object categories\nIntegrates text prompts for customized 3D generation\nPlain English Explanation\nThis research presents a way to turn regular videos into 3D models without needing special camera equipment. The system learns from millions of internet videos, similar to how humans learn about objects by seeing them from different angles.\nThink of it like teaching a computer...\n Click here to read the full summary of this paper",
    "summary": "This article summarizes a research paper detailing a new AI system that generates realistic 3D models from standard videos, without requiring specialized camera equipment.  The system uses a large dataset of internet videos and incorporates zero-shot learning and text prompts for customization.  While the article doesn't specify a release date, it highlights the recent development of this AI technology.\n",
    "image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Farxiv.org%2Fhtml%2F2412.06699v1%2Fx1.png",
    "favicon": "https://media2.dev.to/dynamic/image/width=32,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png"
  },
  {
    "score": 0.13047319650650024,
    "title": "OpenAI whistleblower found dead in San Francisco apartment",
    "id": "https://techcrunch.com/2024/12/13/openai-whistleblower-found-dead-in-san-francisco-apartment/",
    "url": "https://techcrunch.com/2024/12/13/openai-whistleblower-found-dead-in-san-francisco-apartment/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Maxwell Zeff",
    "text": "A former OpenAI employee, Suchir Balaji, was recently found dead in his San Francisco apartment, according to the San Francisco Office of the Chief Medical Examiner. In October, the 26-year-old AI researcher raised concerns about OpenAI breaking copyright law when he was interviewed by The New York Times. \n“The Office of the Chief Medical Examiner (OCME) has identified the decedent as Suchir Balaji, 26, of San Francisco. The manner of death has been determined to be suicide,” said a spokesperson in a statement to TechCrunch. “The OCME has notified the next-of-kin and has no further comment or reports for publication at this time.”\nAfter several years working at OpenAI, Balaji quit the company after realizing that the technology would bring more harm than good to society, he told The New York Times. \nThis was first reported by Mercury News. \n This is a developing story…",
    "summary": "This article is not about recent AI updates.  It reports the death of Suchir Balaji, a former OpenAI employee and whistleblower who raised concerns about OpenAI's copyright practices before his death, which has been ruled a suicide.\n"
  },
  {
    "score": 0.12952937185764313,
    "title": "Study Reveals How AI Models Perform Hidden Step-by-Step Reasoning, Even Without Being Asked",
    "id": "https://dev.to/mikeyoung44/study-reveals-how-ai-models-perform-hidden-step-by-step-reasoning-even-without-being-asked-329p",
    "url": "https://dev.to/mikeyoung44/study-reveals-how-ai-models-perform-hidden-step-by-step-reasoning-even-without-being-asked-329p",
    "publishedDate": "2024-12-13T10:43:35.000Z",
    "author": "Mike Young",
    "text": "This is a Plain English Papers summary of a research paper called Study Reveals How AI Models Perform Hidden Step-by-Step Reasoning, Even Without Being Asked. If you like these kinds of analysis, you should join AImodels.fyi or follow us on Twitter. \nOverview\nResearch investigates hidden computation patterns in chain-of-thought reasoning\nExamines how language models process information between input and output\nUses special filler tokens to track reasoning pathways\nShows language models perform implicit computations even without explicit prompting\nDemonstrates connection between model size and reasoning capabilities\nPlain English Explanation\nLarge language models can solve complex problems by breaking them down into steps, similar to how humans show their work when solving math problems. This process, called chain-of-thought reasoning, h...\n Click here to read the full summary of this paper",
    "summary": "This article summarizes a research paper on how AI models perform step-by-step reasoning, even without explicit prompting.  The study uses special tokens to track the reasoning process within the models, revealing hidden computations.  While it doesn't announce new AI releases, it details a recent advancement in understanding how existing models function, specifically their capacity for chain-of-thought reasoning.  The findings connect model size to reasoning ability.\n",
    "image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Farxiv.org%2Fhtml%2F2412.04537v1%2Fextracted%2F6048742%2Fhidden_tokens_percentage_by_layer.png",
    "favicon": "https://media2.dev.to/dynamic/image/width=32,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png"
  },
  {
    "score": 0.1292649209499359,
    "title": "Liquid AI just raised $250M to develop a more efficient type of AI model",
    "id": "https://techcrunch.com/2024/12/13/liquid-ai-just-raised-250m-to-develop-a-more-efficient-type-of-ai-model/",
    "url": "https://techcrunch.com/2024/12/13/liquid-ai-just-raised-250m-to-develop-a-more-efficient-type-of-ai-model/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Kyle Wiggers",
    "text": "Liquid AI, an AI startup co-founded by robotics luminary Daniela Rus, has raised $250 million in a Series A led by AMD. Per Bloomberg, the round values Liquid AI at over $2 billion.\n Liquid AI aims to build general-purpose AI systems powered by a relatively new type of AI model called a liquid neural network. Liquid neural networks consist of “neurons” governed by equations that predict each individual neuron’s behavior over time. The “liquid” bit in the term “liquid neural networks” refers to the architecture’s flexibility; inspired by the “brains” of roundworms, not only are liquid neural networks much smaller than traditional AI models, but they require far less computing power to run. Liquid AI aims to develop tailored liquid neural networks for applications like e-commerce, consumer electronics, and biotech. As part of AMD’s investment, Liquid AI says it’ll work with the chipmaker to optimize its models for AMD’s GPUs, CPUs, and AI accelerators.",
    "summary": "Liquid AI, co-founded by Daniela Rus, raised $250 million in Series A funding led by AMD.  This brings the company's valuation to over $2 billion.  Their focus is on developing \"liquid neural networks,\" a more efficient AI model inspired by roundworm brains, requiring less computing power than traditional models.  They plan to apply this technology to e-commerce, consumer electronics, and biotech, and will collaborate with AMD to optimize their models for AMD hardware.\n",
    "image": "https://techcrunch.com/wp-content/uploads/2023/06/GettyImages-1452119905.jpg?resize=1200,807",
    "favicon": "https://techcrunch.com/wp-content/uploads/2015/02/cropped-cropped-favicon-gradient.png?w=32"
  },
  {
    "score": 0.12426941841840744,
    "title": "New Study Shows AI Models Still Struggle with Complex Robotic Tasks, Despite Vision and Language Capabilities",
    "id": "https://dev.to/mikeyoung44/new-study-shows-ai-models-still-struggle-with-complex-robotic-tasks-despite-vision-and-language-53jh",
    "url": "https://dev.to/mikeyoung44/new-study-shows-ai-models-still-struggle-with-complex-robotic-tasks-despite-vision-and-language-53jh",
    "publishedDate": "2024-12-13T10:41:10.000Z",
    "author": "Mike Young",
    "text": "This is a Plain English Papers summary of a research paper called New Study Shows AI Models Still Struggle with Complex Robotic Tasks, Despite Vision and Language Capabilities. If you like these kinds of analysis, you should join AImodels.fyi or follow us on Twitter. \nOverview\nThis paper presents a benchmark for evaluating vision, language, and action models on robotic learning tasks.\nThe benchmark includes a suite of tasks that test a model's ability to perceive the environment, understand language, and take appropriate actions.\nThe authors evaluate several state-of-the-art multimodal models on this benchmark and provide insights into their performance and limitations.\nPlain English Explanation\nThe paper focuses on developing a way to  test and compare different AI systems  that can [see, understand language, and take actions](https://aimodels.fyi/papers/arxiv/openvla-o...\n Click here to read the full summary of this paper",
    "summary": "This article summarizes a recent study benchmarking AI models' performance on complex robotic tasks.  The study reveals that even state-of-the-art models, despite advancements in vision and language processing, still struggle with these tasks.  The research focuses on creating a standardized method for testing and comparing AI systems capable of perception, language understanding, and action execution in robotics.  While not announcing a specific new AI release, it highlights current limitations in a crucial area of AI development.\n",
    "image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Farxiv.org%2Fhtml%2F2411.05821v1%2Fextracted%2F5977231%2Famse_all.png",
    "favicon": "https://media2.dev.to/dynamic/image/width=32,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png"
  },
  {
    "score": 0.1210232824087143,
    "title": "Meta asks the US government to block OpenAI’s switch to a for-profit",
    "id": "https://www.theverge.com/2024/12/13/24320880/meta-california-ag-letter-openai-non-profit-elon-musk",
    "url": "https://www.theverge.com/2024/12/13/24320880/meta-california-ag-letter-openai-non-profit-elon-musk",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Alex Heath",
    "text": "Meta is asking California Attorney General Rob Bonta to block OpenAI’s planned transition from a non-profit to for-profit entity. In a letter sent to Bonta’s office this week, Meta says that OpenAI “should not be allowed to flout the law by taking and reappropriating assets it built as a charity and using them for potentially enormous private gains.” The letter, which was first reported on by  The Wall Street Journal   and you can read in full below, goes so far as to say that Meta believes Elon Musk is “qualified and well positioned to represent the interests of Californians in this matter.” Meta supporting Musk’s fight against OpenAI is notable given that Musk and Mark Zuckerberg were talking about literally fighting in a cage match just last year. OpenAI started as a non-profit but stumbled into commercial success with ChatGPT, which now makes billions of dollars a year in revenue. CEO Sam Altman has been clear that the company needs to shed its non-profit status to become more attractive to investors and continuing funding its ambitions. The stakes are so high that OpenAI will have to return the billions of dollars it raised this year (with interest) if it can’t successfully convert to a for-profit company within two years. In its letter to the government, Meta argues that OpenAI’s “conduct could have seismic implications for Silicon Valley” and “represent a paradigm shift for technology startups” by enticing “investors to launch organizations as non-profits, collect hundreds of millions of dollars in tax-free donations to support research and development, and then assume for-profit status as its technology becomes commercially viable.” In response to Meta’s letter, OpenAI board chair Bret Taylor said the company’s non-profit board of directors is “focused on fulfilling our fiduciary obligation by ensuring that the company is well-positioned to continue advancing its mission of ensuring AGI benefits all of humanity.” In a statement shared with The Verge, Taylor said that, “While our work remains ongoing as we continue to consult independent financial and legal advisors, any potential restructuring would ensure the nonprofit continues to exist and thrive, and receives full value for its current stake in the OpenAI for-profit with an enhanced ability to pursue its mission.”  Meta has its own competitive reasons to to hamper OpenAI’s commercial success, of course. Zuckerberg is set on his Meta AI being the most used assistant in the world. He also wants to build AI super intelligence, which OpenAI is racing to make a reality.  Here’s Meta’s full letter to California Attorney General Rob Bonta:   Dear General Bonta: As a California company that builds Generative AI technology, Meta Platforms, Inc. (“Meta”) is deeply concerned about OpenAI’s attempt to shed the non-profit status under which it was founded in order to establish a for-profit entity. We urge you to review this proposed transaction, including the nature and timing of any transfer of assets from OpenAI’s non-profit entity to other entities. Failing to hold OpenAI accountable for its choice to form as a non-profit could lead to a proliferation of similar start-up ventures that are notionally charitable until they are potentially profitable. The People of California have direct and urgent interests in stopping this behavior. All for-profit activities of OpenAI and its related entities should be paused to protect investors and consumers alike. In 2015, OpenAI filed its original certificate of incorporation with the State of Delaware, which reads:  This Corporation shall be a nonprofit corporation organized exclusively for charitable and/or educational purposes within the meaning of section 501(c){3) of the Internal Revenue Code of 1986, as amended, or the corresponding provision of any future United States Internal Revenue law. The specific purpose of this corporation is to provide funding for research, development and distribution of technology related to artificial intelligence... The corporation is not organized for the private gain of any person... The property of this corporation is irrevocably dedicated to the[se] purposes... and no part of the net income or assets of this corporation shall ever inure to the benefit of any director, officer or member thereof or to the benefit of any private person.  OpenAI reaffirmed this commitment on its very own website years later:  Seeing no clear path in the public sector, and given the success of other ambitious projects in private industry, [OpenAI] decided to pursue this project through private means bound by strong commitments to the public good. [OpenAI] initially believed a 501(c)(3) would be the most effective vehicle to direct the development of safe and broadly beneficial AGI while remaining unencumbered by profit incentives.  Taking advantage of this non-profit status, OpenAI raised billions of dollars in capital from investors to further its purported mission. The company represented to the State of California and the world that it would be run without any profit motivation. Investors and the public rightfully relied on that assurance.  Now, OpenAI wants to change its status while retaining all of the benefits that enabled it to reach the point it has today. That is wrong. OpenAI should not be allowed to flout the law by taking and reappropriating assets it built as a charity and using them for potentially enormous private gains.  Moreover, OpenAI’s proposed conversion represents not simply a future, potential abuse of corporate form. We would also urge you to examine whether OpenAI’s past practices are consistent with its obligations as a non-profit – most notably whether it has inappropriately depleted the assets of the non-profit by distributing assets to third-party entities.  OpenAI’s conduct could have seismic implications for Silicon Valley. If permitted, OpenAI’s restructuring would represent a paradigm shift for technology startups; allowing this restructuring would only entice investors to launch organizations as non-profits, collect hundreds of millions of dollars in tax-free donations to support research and development, and then assume for-profit status as its technology becomes commercially viable. Indeed, if OpenAI’s new business model is valid, non-profit investors would get the same for-profit upside as those who invest the conventional way in for-profit companies while also benefiting from tax write-offs bestowed by the government and, ultimately, the public. That would distort the market by essentially requiring any startup seeking to remain competitive to adopt the same playbook.  We understand that Elon Musk and Shivon Zilis are currently seeking to represent the public interests in Musk v. Altman, No. 4:24-cv-04722-YGR (N.D. Cal.). Although we would also urge your office to take direct action, we believe that Mr. Musk and Ms. Zilis are qualified and well positioned to represent the interests of Californians in this matter. Their early, foundational roles in OpenAI’s creation and operations and as prior members of its Board position them to understand better than anyone what OpenAI was intended to be and how its current conduct deviates from its charitable mission. Meta is committed to openness and transparency in the transformative field of AI. OpenAI’s charitable promise to develop safe and broadly beneficial AI free from commercial pressures is an important one, and it should be kept. Given the breakneck speed at which OpenAI is continuing its for-profit conversion, this is a special case with an urgent necessity for action. We appreciate your consideration of our views and are happy to answer any questions you may have. Respectfully,  Meta Platforms",
    "summary": "This article doesn't focus on new AI releases.  Instead, it reports on Meta's request to California's Attorney General to block OpenAI's planned transition from a non-profit to a for-profit company.  Meta argues this transition is unlawful, citing concerns about OpenAI leveraging its non-profit status to gain unfair commercial advantage.  OpenAI's board chair responded, stating their actions are to ensure the company's continued mission.  The rivalry between Meta and OpenAI is a key factor in Meta's opposition.\n"
  },
  {
    "score": 0.12014809995889664,
    "title": "As AI-fueled disinformation explodes, here comes the startup counterattack",
    "id": "https://techcrunch.com/2024/12/13/as-ai-fueled-disinformation-explodes-here-comes-the-startup-counterattack/",
    "url": "https://techcrunch.com/2024/12/13/as-ai-fueled-disinformation-explodes-here-comes-the-startup-counterattack/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Mike Butcher",
    "text": "With disinformation on the rise, especially given the explosion of AI, companies are just as vulnerable to its effects as individuals. Refute is a London-based startup that detects and responds to disinformation on behalf of these commercial entities. It’s now raised a £2.3 million ($2.9 million) pre-seed round led by UK investors Playfair and Episode 1.\nThere are plenty of factors fuelling disinformation attacks, such as geopolitical instability and the use of generative AI to create misleading content. \nMore sophisticated campaigns – often created by state-sponsored or commercial competitors – focus on companies, their supply chains, and even their executives. The result can have massive reputational and financial impact.\nTom Garnett, Co-founder and CEO at Refute told TechCrunch: “We find that existing customers are often using social and media monitoring tools to try and understand the disinformation threat. But these are passive tools generally designed for marketing purposes… In our experience, such signals are misleading, as noise levels are high and the narrative provenance is obscured. The result is that users aren’t equipped to understand—and therefore address—the full picture of the threat.”\nClearly, this is a growing space, as several startups have emerged to go after this market. Competitors to Refute include Alethea (raised $30 million), Blackbird AI ($30.6M), and Logically AI ($36.7M). \nHowever, Garnett claimed: “We provide both the detection and response part of the solution. This sets us apart in the market, as other approaches are primarily focused on detection.”\nRefute finds disinformation campaigns by detecting the so-called “threat actor” behaviors of an adversary. \nGarnett started his career in national security at Detica and BAE Systems, where he built large-scale data analysis solutions to investigate criminal activity with a focus on terrorist attacks. \nHe realized that similar underlying technology could be used to fight criminal activity in the commercial sector including cyber attacks and financial crime: “This led me to head up the government and cybersecurity business at Ripjar: selling, building, and delivering solutions for tech companies, financial services, oil &amp; gas and government customers.”\nHis co-founder Vlad Galu grew up in the 1980s and 1990s Romania, experiencing the threat of disinformation firsthand. He told TechCrunch: “I began building core internet infrastructure and services in Romania and Central-Eastern Europe… Building everything from scratch during a time of emerging technology and limited legal frameworks meant we had to implement layered defense strategies against threats targeting both platforms and end-users.”\nAlso participating in the round was Notion Capital and Amadeus Capital Partners. Refute also received angel investment from investors Charlie Songhurst, Carlos Espinal, James Chappell, and Alastair Paterson.\nAndrew Sheffield, Principal at Playfair Capital said in a statement: “The information landscape is changing: it is harder than ever to tell fact from fiction, and the cost of spreading misleading content continues to fall… Tom and Vlad possess forty-plus years of experience in data analysis, cybersecurity, tackling terrorist attacks, identity management, and money laundering.”",
    "summary": "This article is not about AI updates released today.  It discusses Refute, a London-based startup that has raised £2.3 million to combat AI-fueled disinformation targeting businesses.  Refute's technology detects and responds to disinformation campaigns, differentiating itself from competitors by offering both detection and response capabilities.  The founders' backgrounds in national security and cybersecurity inform their approach.\n"
  },
  {
    "score": 0.11659805476665497,
    "title": "Exxon can’t resist the AI power gold rush",
    "id": "https://techcrunch.com/2024/12/13/exxon-cant-resist-the-ai-power-gold-rush/",
    "url": "https://techcrunch.com/2024/12/13/exxon-cant-resist-the-ai-power-gold-rush/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Tim De Chant",
    "text": "AI continues to reshuffle power and energy markets with even oil giants like Exxon Mobil getting into the mix. \nExxon announced this week that it’s planning to build a power plant for data centers, reflecting just how much electricity tech companies expect they’ll need in the coming decade. According to one estimate, nearly half of new AI data centers might not have enough power by 2027. \nThe oil and gas company already operates power plants for its own operations, but the new project would be its first for outside customers. Though Exxon dabbles in renewable energy, the planned power plant would run on natural gas and generate over 1.5 gigawatts.\nIn a twist, Exxon said that it intends to capture and store over 90% of the carbon dioxide the plant produces.\nThe company isn’t planning to connect the power plant to the grid, avoiding the interconnection backlog that has plagued many new power plants. In an annual strategy document published Wednesday, Exxon described the new project as “reliable, fully-islanded power with no reliance on grid infrastructure.” It did not say where the power plant would be located. Exxon did not reply to a request for comment before publication.\nThe facility should be completed within the next five years, the company told the New York Times. That’s a shorter timeline than most nuclear power plants, which have caught the eye of energy-hungry tech firms. Most of those aren’t scheduled to come online until the early 2030s. \nBut Exxon faces stiffer competition with renewables, which have proven quick to deploy and continue to drop in price. Google’s recently announced renewable energy investment, which including partners will total $20 billion, will start sending electrons to the grid in 2026. Microsoft is contributing to a $5 billion, 9-gigawatt renewable portfolio that has already made its first investment; the inaugural solar project is scheduled to come online six to nine months from now.\nComplicating matters for Exxon is the fact that carbon capture and storage (CCS) adds considerable cost to construction and operation of a fossil fuel power plant. So far, there are only a handful of power plants worldwide that capture some of their carbon pollution, according to the Global CCS Institute, and none of them run on natural gas. That may change given the tax credits available under the Inflation Reduction Act, which offer between $60 to $85 per metric ton of carbon captured and stored.\nStill, the technology has some kinks to work out at the commercial scale. Some have hit their targets, while others have fallen far short. One long-running CCS facility in Canada promised to capture 90% of the carbon dioxide from a small coal plant, yet after nearly a decade in operation, it managed to capture just under 60%, according to the Institute for Energy Economics and Financial Analysis.\nMost Popular\n \nTim De Chant is a senior climate reporter at TechCrunch. He has written for a wide range of publications, including Wired magazine, the Chicago Tribune, Ars Technica, The Wire China, and NOVA Next, where he was founding editor. De Chant is also a lecturer in MIT’s Graduate Program in Science Writing, and he was awarded a Knight Science Journalism Fellowship at MIT in 2018, during which time he studied climate technologies and explored new business models for journalism. He received his PhD in environmental science, policy, and management from the University of California, Berkeley, and his BA degree in environmental studies, English, and biology from St. Olaf College.\t\nView Bio \nNewsletters\nSubscribe for the industry’s biggest tech news\nRelated\nLatest in Climate",
    "summary": "This article is not about AI updates released today.  It discusses ExxonMobil's plan to build a 1.5-gigawatt natural gas power plant to supply data centers, driven by the increasing electricity demands of AI.  The plant aims for over 90% carbon dioxide capture and storage, and is expected to be completed within five years.  The article contrasts Exxon's approach with Google and Microsoft's investments in renewable energy to power AI data centers.\n"
  },
  {
    "score": 0.11381836235523224,
    "title": "AI Text Generation: How Small Choices Create Different Outcomes in Language Models",
    "id": "https://dev.to/mikeyoung44/ai-text-generation-how-small-choices-create-different-outcomes-in-language-models-1bb4",
    "url": "https://dev.to/mikeyoung44/ai-text-generation-how-small-choices-create-different-outcomes-in-language-models-1bb4",
    "publishedDate": "2024-12-13T10:42:22.000Z",
    "author": "Mike Young",
    "text": "This is a Plain English Papers summary of a research paper called AI Text Generation: How Small Choices Create Different Outcomes in Language Models. If you like these kinds of analysis, you should join AImodels.fyi or follow us on Twitter. \nOverview\nResearch exploring how language models make text generation choices\nAnalysis of probability distributions in text generation\nNovel framework for understanding generation uncertainty\nStudy of how small changes affect generation outcomes\nExamination of \"forking paths\" in neural text generation\nPlain English Explanation\nText generation by AI models works like a series of crossroads. At each point where the model needs to choose the next word, it faces multiple possible paths. This research examines how these choices branch out and affect the final text.\nThink of it like a complex game of \"Cho...\n Click here to read the full summary of this paper",
    "summary": "This article summarizes a research paper on AI text generation, not the latest AI releases.  The paper explores how small choices within language models lead to vastly different outputs, illustrating the concept using the metaphor of branching paths in a decision tree.  It does not provide information on newly released AI technologies.\n",
    "image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Farxiv.org%2Fhtml%2F2412.07961v1%2Fx1.png",
    "favicon": "https://media2.dev.to/dynamic/image/width=32,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png"
  },
  {
    "score": 0.1087796688079834,
    "title": "Sam Altman and Jeff Bezos are the latest billionaires to donate $1M to Trump fund",
    "id": "https://techcrunch.com/2024/12/13/sam-altman-and-jeff-bezos-are-the-latest-billionaires-to-donate-1m-to-trump-fund/",
    "url": "https://techcrunch.com/2024/12/13/sam-altman-and-jeff-bezos-are-the-latest-billionaires-to-donate-1m-to-trump-fund/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Rebecca Bellan",
    "text": "OpenAI CEO Sam Altman and Jeff Bezos’ Amazon plan to donate $1 million each to President-elect Donald Trump’s inaugural fund, according to reports from Fox and the Wall Street Journal. \nTechCrunch has confirmed Altman’s plans to personally commit the money, which is not coming directly from OpenAI. \nThe donations from the billionaires follow plans by Mark Zuckerberg’s Meta to also donate $1 million to Trump’s inauguration fund as America’s most powerful tech leaders cosy up to the incoming administration. Inaugural funds are used for activities related to the president’s inauguration ceremony in January.\nAmazon’s donation is being prepared as Bezos, the company’s executive chairman, heads over to Mar-a-Lago to visit with Trump next week, the Journal reports. Bezos and Trump had a fraught relationship in the past. During his first term, Trump repeatedly criticized Amazon’s business practices and attacked The Washington Post, which Bezos owns, for being critical of his administration. \nThis time around, Bezos has made a concerted effort to heal those ties. Bezos blocked the Post from endorsing Vice President Kamala Harris for president, and has congratulated Trump for his “extraordinary political comeback” on X. \nAltman, who hasn’t faced public criticism from Trump yet, told TechCrunch in a statement: “President Trump will lead our country into the age of AI, and I am eager to support his efforts to ensure America stays ahead.”\nSilicon Valley largely expects Trump to be light on AI regulation, which they say is necessary for the U.S. to remain competitive on a global scale. And while Altman has no known beef with Trump, the OpenAI founder is in the middle of a legal battle with Elon Musk, another close Trump ally, over OpenAI’s attempts to transition to a for-profit company.\nMost Popular\n \nRebecca Bellan covers transportation for TechCrunch. She’s interested in all things micromobility, EVs, AVs, smart cities, AI, sustainability and more. Previously, she covered social media for Forbes.com, and her work has appeared in Bloomberg CityLab, The Atlantic, The Daily Beast, Mother Jones, i-D (Vice) and more.\nRebecca studied journalism and history at Boston University. She has invested in Ethereum.\t\nView Bio \nNewsletters\nSubscribe for the industry’s biggest tech news\nRelated\nLatest in Government &amp; Policy",
    "summary": "This article discusses Sam Altman (OpenAI CEO) and Jeff Bezos's $1 million donations to Donald Trump's inaugural fund.  It does not contain any information about recent AI updates.\n",
    "image": "https://techcrunch.com/wp-content/uploads/2024/12/bezos-altman-trump.jpg?resize=1200,800",
    "favicon": "https://techcrunch.com/wp-content/uploads/2015/02/cropped-cropped-favicon-gradient.png?w=32"
  },
  {
    "score": 0.1070118099451065,
    "title": "How AI is Revolutionizing Software Development: A Journey for Every Developer",
    "id": "https://dev.to/praveenrajamani/how-ai-is-revolutionizing-software-development-a-journey-for-every-developer-4pk0",
    "url": "https://dev.to/praveenrajamani/how-ai-is-revolutionizing-software-development-a-journey-for-every-developer-4pk0",
    "publishedDate": "2024-12-13T16:05:17.000Z",
    "author": "Praveen",
    "text": "The landscape of software development is undergoing a remarkable transformation, driven by the rapid evolution of Artificial Intelligence. Whether you're a beginner or a seasoned senior developer, AI is set to revolutionize how we create, test, and deploy software.\n What AI Means for Developers \nAI isn't here to replace developers but to empower them. Recent studies show that programmers using AI can complete 126% more projects per week. This is not about job replacement; it's about augmenting human creativity and efficiency.\n Key AI Capabilities in Software Development \nAI is already improving several key areas of software development:\n  Code Generation: The facility provided by tools like GitHub Copilot will be able to generate code snippets for you and thus help in writing more efficiently. \n  Intelligent Debugging: AI can itself check the occurrence of errors in the codes and automatically fix them, therefore reducing debugging time significantly. \n  Smart Testing: AI-driven frameworks can automate testing, predict problems, and increase the quality of the software. \n The Evolving Developer Role \nAs AI tools continue to improve, the role of the developer will evolve from pure coders to strategic technology orchestrators. Among others, your role will increasingly involve:\n Guiding AI tools \n Solving complex architectural challenges \n Focusing on innovative problem-solving \n Emphasizing innovative problem-solving \n Ensuring ethical and strategic in the implementation of technology \n Preparing for the AI-Driven Future \nPro Tips for Developers:\n Use AI as a collaboration tool \n Learn continuously, adapt \n Develop AI integration and oversight skills \n Provide focus on strategic thinking and creativity \n Looking Ahead: The Role of AI in Software Development by 2027 \nBy 2027, 70% of developers will leverage AI-powered coding tools. The future is not about competition with AI but alongside to build even more sophisticated and efficient solutions with the help of AI. AI is a strong assistant and an amplifier of your capabilities. Stay curious, keep learning, and be ready to ride the wave of technological innovation!\n  What AI tools do you use in your development process? Let’s discuss in the comments below!",
    "summary": "This article discusses how AI is transforming software development, not recent AI tool releases.  Therefore, I cannot answer your query using this text.\n",
    "image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fgw5wvgxij4up2gtc7bxr.png",
    "favicon": "https://media2.dev.to/dynamic/image/width=32,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png"
  },
  {
    "score": 0.10380373150110245,
    "title": "WhatsApp lets you select specific people within a group to start a group call without disturbing anyone",
    "id": "https://techcrunch.com/2024/12/13/whatsapp-lets-you-select-specific-people-within-a-group-to-start-a-group-call-without-disturbing-anyone/",
    "url": "https://techcrunch.com/2024/12/13/whatsapp-lets-you-select-specific-people-within-a-group-to-start-a-group-call-without-disturbing-anyone/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Ivan Mehta",
    "text": "WhatsApp announced that it had added new video calling features just before the holidays, including participant selection for group video calls, better resolution, and a revamped call tab on the desktop. This is another step from Meta in making WhatsApp a viable option for both personal and work calls instead of using Google Meet or Zoom. \nWhen you’re in a group chat, the messaging app already allows you to place a call to the whole group. However, at times, you don’t want or need to call all the people in the group and disturb them. Now, the company is rolling out an option to select participants when placing a group call. This is handy when you know only a select number of people are available to talk, or you are planning a surprise for someone in the group.\n \nWhatsApp is also adding new video call effects, including puppy ears, taking you underwater, or handing you a microphone for karaoke.\nAs for the desktop app, the company is redesigning its call tab to prominently display buttons to start a call, create a new call link, and call a number. The company said it is also improving resolution across both one-on-one and group calls on both mobile and desktop.\nIn the past year, WhatsApp has improved its video call feature by adding support for up to 32 participants, introducing a speaker spotlight to highlight the current speaker on a call with multiple people, and adding screen sharing with audio. Many organizations use WhatsApp for internal communication, and all these features can help when it comes to work-related video calls on WhatsApp.\nMost Popular\n \nIvan covers global consumer tech developments at TechCrunch. He is based out of India and has previously worked at publications including Huffington Post and The Next Web. You can reach out to him at im[at]ivanmehta[dot]com\t\nView Bio \nNewsletters\nSubscribe for the industry’s biggest tech news\nRelated\nLatest in Apps",
    "summary": "This article is about WhatsApp's new video calling features, not AI updates.  Therefore, it does not contain information relevant to your search query.\n",
    "image": "https://techcrunch.com/wp-content/uploads/2024/12/whatsapp-logo.jpg?resize=1200,800",
    "favicon": "https://techcrunch.com/wp-content/uploads/2015/02/cropped-cropped-favicon-gradient.png?w=32"
  },
  {
    "score": 0.11758024245500565,
    "title": "Databricks is on track to raise a record $9.5+ billion round at $60B valuation",
    "id": "https://techcrunch.com/2024/12/13/databricks-is-on-track-to-raise-a-record-9-5-billion-round-at-60b-valuation/",
    "url": "https://techcrunch.com/2024/12/13/databricks-is-on-track-to-raise-a-record-9-5-billion-round-at-60b-valuation/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Julie Bort",
    "text": "Posted:\n 12:59 PM PST · December 13, 2024 \n  Image Credits:Getty Images \n Databricks is close to finalizing a $9.5 billion round at a $60 billion valuation, including a secondary sale for employees that could climb even higher before it closes, Reuters reports. Just 17 days ago, Reuters reported that the deal was at $8 billion and a $55 billion valuation. Thrive Capital is leading the deal with and returning investors Andreessen Horowitz, Insight Partners, and Singaporean sovereign wealth fund GIC. This is bigger than OpenAI’s $6.6 billion raise in October, the largest round of all time  and was also led by Joshua Kushner’s Thrive (pictured). Databricks might also take on $4.5 billion of debt as part of this deal. \nDespite the jaw-dropping numbers, VCs feel that the reported $92.50 per share is a bargain given Databricks’ biggest competitor is Snowflake, one of tech’s most successful IPOs. That company currently has a market cap around $56 billion.\nNewsletters\nSubscribe for the industry’s biggest tech news\nRelated\nLatest in Enterprise",
    "summary": "This article is about Databricks' large funding round, not AI updates.  The article reports that Databricks is close to securing a $9.5 billion investment round, valuing the company at $60 billion.  This surpasses OpenAI's recent record-breaking funding round.  There is no mention of AI updates in this article.\n"
  },
  {
    "score": 0.11127818375825882,
    "title": "See what Google’s Project Astra AR glasses can do (for a select few beta testers)",
    "id": "https://techcrunch.com/video/see-what-googles-project-astra-ar-glasses-can-do-for-a-select-few-beta-testers/",
    "url": "https://techcrunch.com/video/see-what-googles-project-astra-ar-glasses-can-do-for-a-select-few-beta-testers/",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Maggie Stamets",
    "text": "11:32 AM PST · December 13, 2024 \n \nGoogle has released a prototype of Project Astra’s AR glasses for testing in the real world. The glasses are part of Googles long-term plan to one day have hardware with augmented reality and multimodal AI capabilities. In the meantime, they will be releasing demos to get the attention of consumers, developers, and their competition.\nAlong with the the glasses, Google also announced their new operating system Android XR which will allow developers to build experiences for XR hardware like glasses and headsets. The OS will be powering the Project Atra Glasses along with the newly announced Samsung-built headset Project Moohan which will be for sale next year.\nMost Popular\nMaggie Stamets is a Podcast Producer for TechCrunch based in Denver, Colorado. Previously, she worked as the Brand and Content Manager for BUILT BY GIRLS where she developed an interest in tech and a passion for creating equitable and welcoming professional tech spaces. She holds a bachelor’s degree in Journalism with a minor in English from Hofstra University in New York.\nView Bio \nNewsletters\nSubscribe for the industry’s biggest tech news\nRelated",
    "summary": "Google released a prototype of its Project Astra AR glasses for beta testing.  These glasses incorporate augmented reality and multimodal AI capabilities, part of Google's long-term hardware strategy.  Additionally, Google announced Android XR, a new operating system for XR devices like the Project Astra glasses and the upcoming Samsung Project Moohan headset.  While not explicitly stated as \"latest AI updates,\" the AI capabilities within the glasses represent a development in the field.\n"
  },
  {
    "score": 0.11101903766393661,
    "title": "Leak: Lenovo’s larger Legion handheld adds OLED and keeps the detachable mouse",
    "id": "https://www.theverge.com/2024/12/13/24320607/lenovo-legion-go-8-leak-gamepad",
    "url": "https://www.theverge.com/2024/12/13/24320607/lenovo-legion-go-8-leak-gamepad",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Sean Hollister",
    "text": "Earlier today, Evan Blass revealed an unannounced Lenovo Legion Go S handheld gaming PC with an extremely intriguing twist: a Steam button that suggests it could be the first third-party SteamOS handheld, and thus the first true competitor to the Steam Deck. But that handheld gaming PC apparently won’t be alone: Blass just provided The Verge with these images of a new, larger Lenovo Legion Go as well.  As you can see, this Legion Go plans to keep the detachable Joy-Con like gamepads and kickstand that were the single most distinctive features of the original — and they’ll keep the “FPS mode” where you can plop one of those controllers on a disc-shaped skate and use it like a vertical mouse.          But one model may also swap out the Legion Go’s 8.8-inch IPS LCD screen for one with an OLED panel at the same size, according to the original filenames of these images. That should mean improved colors and deeper blacks, and potentially improved response times. We don’t have any specs or marketing claims yet, though, like the all-important battery life size. Nor have we gotten a glimpse of the ports on this system. There is an AMD Z2 Extreme chip coming that could likely be the core of this handheld, but we don’t yet know.          In my review of the original Legion Go, I was mixed on the detachable controllers with their sharp-ish edges and loads of extra mouse buttons that made them awkward to hold. These one seem to be far more smoothly sculpted, though, with revised mouse buttons on the right detachable pad, and a cover you can place over the mounting rail so the copper charging pins don’t poke your palm. The images we’ve seen do not feature a Steam button, so it’s quite likely Lenovo is still hedging its bets with Windows in addition to SteamOS. But we are much more confident in our prediction that the smaller Lenovo Legion Go S will be a SteamOS handheld. Blass showed me filenames that suggest the S will be “powered by” Steam, mirroring Valve’s new branding guidelines for “Powered by SteamOS” devices.  Valve defines “Powered by SteamOS” as “hardware running the SteamOS operating system, implemented in close collaboration with Valve.”  We’ve reached out to Valve and Lenovo to hopefully learn more.",
    "summary": "This article discusses leaks of new Lenovo Legion Go handheld gaming PCs.  There's no mention of AI updates.\n"
  },
  {
    "score": 0.09683027863502502,
    "title": "What’s your favorite framework for building GenAI applications? (LangChain, Haystack, LlamaIndex, or others?) 🚀",
    "id": "https://dev.to/eze_lanza/whats-your-favorite-framework-for-building-genai-applications-langchain-haystack-llamaindex-p6k",
    "url": "https://dev.to/eze_lanza/whats-your-favorite-framework-for-building-genai-applications-langchain-haystack-llamaindex-p6k",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Eze Lanza",
    "text": "Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's permalink.",
    "summary": "This webpage is a discussion forum post asking developers about their preferred framework for building generative AI applications (LangChain, Haystack, LlamaIndex, etc.).  It does not contain information about the latest AI updates released today.\n"
  },
  {
    "score": 0.1330806314945221,
    "title": "Llama 3.3 vs OpenAI O1",
    "id": "https://dev.to/ashinno/llama-33-vs-openai-o1-623",
    "url": "https://dev.to/ashinno/llama-33-vs-openai-o1-623",
    "publishedDate": "2024-12-13T00:00:00.000Z",
    "author": "Ash Inno",
    "text": "Two of the most talked-about advancements in this space are Llama 3.3 and OpenAI O1. These models bring innovative capabilities to the table, promising developers unprecedented control, accuracy, and performance.\nBut which one should you choose for your next project? In this blog post, we’ll explore these two AI titans, compare their features, and evaluate their performance in real-world scenarios. Along the way, we’ll also highlight a tool every developer should know about: Apidog, a powerful API development solution that integrates seamlessly with AI models. Download Apidog for free to streamline your API workflows and unlock the full potential of Llama 3.3 or OpenAI O1.\nUnderstanding Llama 3.3: A New Frontier in AI\n Llama 3.3, the latest iteration from Meta AI, builds on its predecessor’s strengths with improved reasoning capabilities, better contextual understanding, and a developer-friendly API. Designed to compete with OpenAI’s offerings, Llama 3.3 focuses on:\nEnhanced Reasoning Abilities: Thanks to advanced training techniques, it excels in tasks requiring logical and deductive reasoning.\nCustomizable Prompt Formats: Developers can create tailored experiences by customizing prompt formats, a feature highlighted in its official documentation.\nScalability: Whether you’re running lightweight applications or handling enterprise-level demands, Llama 3.3’s scalable architecture delivers consistent performance.\n   \nWhy Developers Love Llama 3.3\nOne of the standout features of Llama 3.3 is its open-source flexibility. Unlike some proprietary models, it allows developers to tweak and experiment with its API, fostering innovation. Additionally, Llama 3.3’s superior natural language generation capabilities make it a go-to choice for tasks like content creation, sentiment analysis, and chatbots.\n   \n Pro Tip: Pair Llama 3.3 with Apidog to design, test, and debug your APIs effortlessly. This combination ensures smoother integration and faster deployment.\nWhat is OpenAI o1?\nOn the other hand,  OpenAI O1  represents a leap forward in reasoning capabilities, designed to tackle intricate challenges across domains. It offers unparalleled support for tasks like strategic ideation, coding exercises, UX-to-code prototyping, and advanced educational scenarios. Compared to its predecessors, the o1 model shines with deeper logical reasoning and contextual comprehension, making it ideal for:\nDynamic Adaptability: Whether it’s customer service, coding assistance, or creative writing, O1 adapts seamlessly to various use cases.\nEnhanced Learning-to-Reason Capabilities: OpenAI has prioritized reasoning with O1, as detailed in this resource.\nEase of Use: Its API is designed with simplicity in mind, allowing developers of all skill levels to leverage its power.\n   \nKey Features of OpenAI O1\nFine-Tuning Capabilities: OpenAI O1 allows you to fine-tune the model for specific tasks, offering unmatched precision.\nRobust Ecosystem: With support for tools like OpenAI Playground and integrations with third-party platforms, O1 makes AI accessible to everyone.\nAdvanced Content Filtering: To ensure safe and ethical usage, OpenAI O1 incorporates state-of-the-art content moderation tools.\n Did You Know? OpenAI O1’s API documentation is optimized for quick onboarding, so you can start building applications in minutes. Pairing it with Apidog can further enhance your development experience.\nLlama 3.3 vs OpenAI O1: Feature-by-Feature Comparison\n1. Performance and Accuracy\nLlama 3.3: Excels in structured tasks like logical reasoning and data analysis. It’s a favorite for scenarios requiring deep contextual understanding.\nOpenAI O1: Offers a balanced performance across creative, logical, and conversational tasks, making it versatile.\n2. API Design and Usability\nLlama 3.3: Its API is flexible but requires some familiarity with open-source frameworks.\nOpenAI O1: Features an intuitive API design that’s beginner-friendly and well-documented.\n3. Cost-Effectiveness\nLlama 3.3: Being open-source, it offers cost advantages, especially for large-scale projects.\nOpenAI O1: While it comes with subscription costs, its robust support and features justify the investment for many businesses.\n4. Security and Compliance\nLlama 3.3: Offers customizable security measures but requires developer intervention.\nOpenAI O1: Comes with built-in compliance features, including GDPR adherence and advanced content filtering.\nHere's an updated version of your content with a pricing section, including the note that the API pricing of O1 is not available yet:\n5. Pricing\nLlama3.3 Pricing\nFrom our perspective, this pricing makes Llama 3.3 particularly attractive for developers working on a budget or scaling projects with high token demands. Whether you’re running it locally or through a platform, these competitive rates ensure that high-quality AI is accessible without breaking the bank.\nInput Token Costs\nFor processing 1 million input tokens, Llama 3.3 is priced at $0.1, which matches its predecessor, Llama 3.1 70B. Compared to other models, this is one of the lowest rates:\nAmazon Nova Pro: $0.80 – Eight times more expensive for the same volume.\nGPT-4o: $2.5 – Significantly higher than Llama 3.3.\nClaude 3.5 Sonnet: $3.0 – Also much more expensive.\nThis low input cost makes Llama 3.3 particularly suitable for applications that require processing large amounts of text, such as chatbots or text analysis tools.\nOutput Token Costs\nFor 1 million output tokens, Llama 3.3 is priced at $0.4, again matching the cost of Llama 3.1 70B. Here’s how it compares:\nAmazon Nova Pro: $1.8 – Over four times higher than Llama 3.3.\nGPT-4o: $10.0 – Considerably higher.\nClaude 3.5 Sonnet: $15.0 – Makes Llama 3.3 a highly economical choice.\nO1 API Pricing\nAt this date, the API pricing for O1 is not available yet. We will update this section as soon as pricing details are announced.\nReal-World Use Cases\nContent Creation\nLlama 3.3: Ideal for generating long-form articles, crafting social media posts, or automating email campaigns.\nOpenAI O1: Shines in creative writing, story generation, and brainstorming sessions.\nCustomer Support\nLlama 3.3: Its reasoning capabilities make it suitable for handling complex queries.\nOpenAI O1: Excels in empathetic and conversational interactions, thanks to its human-like tone.\nSoftware Development\nLlama 3.3: Great for debugging and optimizing code snippets.\nOpenAI O1: A reliable partner for code generation, refactoring, and documentati",
    "summary": "This article compares two recent AI models: Llama 3.3 from Meta AI and OpenAI O1.  Llama 3.3 focuses on enhanced reasoning, customizable prompts, and scalability, while OpenAI O1 excels in complex tasks like strategic ideation and coding.  Neither release date is specified in the article, so it cannot confirm if they are the latest AI updates released *today*.\n"
  }
]